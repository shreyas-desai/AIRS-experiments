{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "892543ae-da5a-44d3-95e4-1e3e47f7c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Tuple, List, Dict\n",
    "from pathlib import Path\n",
    "import pkg_resources\n",
    "import pickle\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from typing import Dict\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9360f4db-3f83-4afa-916c-0917c80daf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularizers\n",
    "class Regularizer(nn.Module, ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, factors: Tuple[torch.Tensor]):\n",
    "        pass\n",
    "\n",
    "class N3(Regularizer):\n",
    "    def __init__(self, weight: float):\n",
    "        super(N3, self).__init__()\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, factors):\n",
    "        norm = 0\n",
    "        for f in factors:\n",
    "            norm += self.weight * torch.sum(torch.abs(f) ** 3)\n",
    "        return norm / factors[0].shape[0]\n",
    "\n",
    "\n",
    "class Lambda3(Regularizer):\n",
    "    def __init__(self, weight: float):\n",
    "        super(Lambda3, self).__init__()\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, factor):\n",
    "        ddiff = factor[1:] - factor[:-1]\n",
    "        rank = int(ddiff.shape[1] / 2)\n",
    "        diff = torch.sqrt(ddiff[:, :rank]**2 + ddiff[:, rank:]**2)**3\n",
    "        return self.weight * torch.sum(diff) / (factor.shape[0] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75d96f8d-2fd2-47f9-aceb-0968d7172eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Abstract class\n",
    "class TKBCModel(nn.Module, ABC):\n",
    "    @abstractmethod\n",
    "    def get_rhs(self, chunk_begin: int, chunk_size: int):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_queries(self, queries: torch.Tensor):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def score(self, x: torch.Tensor):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward_over_time(self, x: torch.Tensor):\n",
    "        pass\n",
    "\n",
    "    def get_ranking(\n",
    "            self, queries: torch.Tensor,\n",
    "            filters: Dict[Tuple[int, int, int], List[int]],\n",
    "            batch_size: int = 1000, chunk_size: int = -1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns filtered ranking for each queries.\n",
    "        :param queries: a torch.LongTensor of quadruples (lhs, rel, rhs, timestamp)\n",
    "        :param filters: filters[(lhs, rel, ts)] gives the elements to filter from ranking\n",
    "        :param batch_size: maximum number of queries processed at once\n",
    "        :param chunk_size: maximum number of candidates processed at once\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if chunk_size < 0:\n",
    "            chunk_size = self.sizes[2]\n",
    "        ranks = torch.ones(len(queries))\n",
    "        with torch.no_grad():\n",
    "            c_begin = 0\n",
    "            while c_begin < self.sizes[2]:\n",
    "                b_begin = 0\n",
    "                rhs = self.get_rhs(c_begin, chunk_size)\n",
    "                while b_begin < len(queries):\n",
    "                    these_queries = queries[b_begin:b_begin + batch_size]\n",
    "                    q = self.get_queries(these_queries)\n",
    "\n",
    "                    scores = q @ rhs\n",
    "                    targets = self.score(these_queries)\n",
    "                    assert not torch.any(torch.isinf(scores)), \"inf scores\"\n",
    "                    assert not torch.any(torch.isnan(scores)), \"nan scores\"\n",
    "                    assert not torch.any(torch.isinf(targets)), \"inf targets\"\n",
    "                    assert not torch.any(torch.isnan(targets)), \"nan targets\"\n",
    "\n",
    "                    # set filtered and true scores to -1e6 to be ignored\n",
    "                    # take care that scores are chunked\n",
    "                    for i, query in enumerate(these_queries):\n",
    "                        filter_out = filters[(query[0].item(), query[1].item(), query[3].item())]\n",
    "                        filter_out += [queries[b_begin + i, 2].item()]\n",
    "                        if chunk_size < self.sizes[2]:\n",
    "                            filter_in_chunk = [\n",
    "                                int(x - c_begin) for x in filter_out\n",
    "                                if c_begin <= x < c_begin + chunk_size\n",
    "                            ]\n",
    "                            scores[i, torch.LongTensor(filter_in_chunk)] = -1e6\n",
    "                        else:\n",
    "                            scores[i, torch.LongTensor(filter_out)] = -1e6\n",
    "                    ranks[b_begin:b_begin + batch_size] += torch.sum(\n",
    "                        (scores >= targets).float(), dim=1\n",
    "                    ).cpu()\n",
    "\n",
    "                    b_begin += batch_size\n",
    "\n",
    "                c_begin += chunk_size\n",
    "        return ranks\n",
    "\n",
    "    def get_auc(\n",
    "            self, queries: torch.Tensor, batch_size: int = 1000\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns filtered ranking for each queries.\n",
    "        :param queries: a torch.LongTensor of quadruples (lhs, rel, rhs, begin, end)\n",
    "        :param batch_size: maximum number of queries processed at once\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        all_scores, all_truth = [], []\n",
    "        all_ts_ids = None\n",
    "        with torch.no_grad():\n",
    "            b_begin = 0\n",
    "            while b_begin < len(queries):\n",
    "                these_queries = queries[b_begin:b_begin + batch_size]\n",
    "                scores = self.forward_over_time(these_queries)\n",
    "                all_scores.append(scores.cpu().numpy())\n",
    "                if all_ts_ids is None:\n",
    "                    all_ts_ids = torch.arange(0, scores.shape[1]).cuda()[None, :]\n",
    "                assert not torch.any(torch.isinf(scores) + torch.isnan(scores)), \"inf or nan scores\"\n",
    "                truth = (all_ts_ids <= these_queries[:, 4][:, None]) * (all_ts_ids >= these_queries[:, 3][:, None])\n",
    "                all_truth.append(truth.cpu().numpy())\n",
    "                b_begin += batch_size\n",
    "\n",
    "        return np.concatenate(all_truth), np.concatenate(all_scores)\n",
    "\n",
    "    def get_time_ranking(\n",
    "            self, queries: torch.Tensor, filters: List[List[int]], chunk_size: int = -1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns filtered ranking for a batch of queries ordered by timestamp.\n",
    "        :param queries: a torch.LongTensor of quadruples (lhs, rel, rhs, timestamp)\n",
    "        :param filters: ordered filters\n",
    "        :param chunk_size: maximum number of candidates processed at once\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if chunk_size < 0:\n",
    "            chunk_size = self.sizes[2]\n",
    "        ranks = torch.ones(len(queries))\n",
    "        with torch.no_grad():\n",
    "            c_begin = 0\n",
    "            q = self.get_queries(queries)\n",
    "            targets = self.score(queries)\n",
    "            while c_begin < self.sizes[2]:\n",
    "                rhs = self.get_rhs(c_begin, chunk_size)\n",
    "                scores = q @ rhs\n",
    "                # set filtered and true scores to -1e6 to be ignored\n",
    "                # take care that scores are chunked\n",
    "                for i, (query, filter) in enumerate(zip(queries, filters)):\n",
    "                    filter_out = filter + [query[2].item()]\n",
    "                    if chunk_size < self.sizes[2]:\n",
    "                        filter_in_chunk = [\n",
    "                            int(x - c_begin) for x in filter_out\n",
    "                            if c_begin <= x < c_begin + chunk_size\n",
    "                        ]\n",
    "                        max_to_filter = max(filter_in_chunk + [-1])\n",
    "                        assert max_to_filter < scores.shape[1], f\"fuck {scores.shape[1]} {max_to_filter}\"\n",
    "                        scores[i, filter_in_chunk] = -1e6\n",
    "                    else:\n",
    "                        scores[i, filter_out] = -1e6\n",
    "                ranks += torch.sum(\n",
    "                    (scores >= targets).float(), dim=1\n",
    "                ).cpu()\n",
    "\n",
    "                c_begin += chunk_size\n",
    "        return ranks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06da9af0-a833-4851-8cc8-5449cdf44abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "class TemporalDataset(object):\n",
    "    def __init__(self, name: str):\n",
    "        self.root = Path(DATA_PATH) / name\n",
    "        self.entity_map = {}\n",
    "        self.rel_map = {}\n",
    "        self.ts_map = {}\n",
    "        with open(self.root/\"ent_id\",'r',encoding=\"utf-8\") as file:\n",
    "            for row in file.readlines():\n",
    "                ent, code = row.split('\\t')\n",
    "                self.entity_map[code.strip()] = ent\n",
    "                \n",
    "        with open(self.root/\"rel_id\",'r',encoding=\"utf-8\") as file:\n",
    "            for row in file.readlines():\n",
    "                rel, code = row.split('\\t')\n",
    "                self.rel_map[code.strip()] = rel\n",
    "\n",
    "        with open(self.root/\"ts_id\",'r',encoding=\"utf-8\") as file:\n",
    "            for row in file.readlines():\n",
    "                ts, code = row.split('\\t')\n",
    "                self.ts_map[code.strip()] = ts\n",
    "\n",
    "        self.data = {}\n",
    "        for f in ['train', 'test', 'valid']:\n",
    "            in_file = open(str(self.root / (f + '.pickle')), 'rb')\n",
    "            self.data[f] = pickle.load(in_file)\n",
    "\n",
    "        maxis = np.max(self.data['train'], axis=0)\n",
    "        self.n_entities = int(max(maxis[0], maxis[2]) + 1)\n",
    "        self.n_predicates = int(maxis[1] + 1)\n",
    "        self.n_predicates *= 2\n",
    "        if maxis.shape[0] > 4:\n",
    "            self.n_timestamps = max(int(maxis[3] + 1), int(maxis[4] + 1))\n",
    "        else:\n",
    "            self.n_timestamps = int(maxis[3] + 1)\n",
    "        try:\n",
    "            inp_f = open(str(self.root / f'ts_diffs.pickle'), 'rb')\n",
    "            self.time_diffs = torch.from_numpy(pickle.load(inp_f)).cuda().float()\n",
    "            # print(\"Assume all timestamps are regularly spaced\")\n",
    "            # self.time_diffs = None\n",
    "            inp_f.close()\n",
    "        except OSError:\n",
    "            print(\"Assume all timestamps are regularly spaced\")\n",
    "            self.time_diffs = None\n",
    "\n",
    "        try:\n",
    "            e = open(str(self.root / f'event_list_all.pickle'), 'rb')\n",
    "            self.events = pickle.load(e)\n",
    "            e.close()\n",
    "\n",
    "            f = open(str(self.root / f'ts_id'), 'rb')\n",
    "            dictionary = pickle.load(f)\n",
    "            f.close()\n",
    "            self.timestamps = sorted(dictionary.keys())\n",
    "        except OSError:\n",
    "            print(\"Not using time intervals and events eval\")\n",
    "            self.events = None\n",
    "\n",
    "        if self.events is None:\n",
    "            inp_f = open(str(self.root / f'to_skip.pickle'), 'rb')\n",
    "            self.to_skip: Dict[str, Dict[Tuple[int, int, int], List[int]]] = pickle.load(inp_f)\n",
    "            inp_f.close()\n",
    "\n",
    "\n",
    "        # If dataset has events, it's wikidata.\n",
    "        # For any relation that has no beginning & no end:\n",
    "        # add special beginning = end = no_timestamp, increase n_timestamps by one.\n",
    "\n",
    "    def has_intervals(self):\n",
    "        return self.events is not None\n",
    "\n",
    "    def get_examples(self, split):\n",
    "        return self.data[split]\n",
    "\n",
    "    def get_train(self):\n",
    "        copy = np.copy(self.data['train'])\n",
    "        tmp = np.copy(copy[:, 0])\n",
    "        copy[:, 0] = copy[:, 2]\n",
    "        copy[:, 2] = tmp\n",
    "        copy[:, 1] += self.n_predicates // 2  # has been multiplied by two.\n",
    "        return np.vstack((self.data['train'], copy))\n",
    "\n",
    "    def eval(\n",
    "            self, model: TKBCModel, split: str, n_queries: int = -1, missing_eval: str = 'both',\n",
    "            at: Tuple[int] = (1, 3, 10)\n",
    "    ):\n",
    "        if self.events is not None:\n",
    "            return self.time_eval(model, split, n_queries, 'rhs', at)\n",
    "        test = self.get_examples(split)\n",
    "        examples = torch.from_numpy(test.astype('int64')).cuda()\n",
    "        missing = [missing_eval]\n",
    "        if missing_eval == 'both':\n",
    "            missing = ['rhs', 'lhs']\n",
    "\n",
    "        mean_reciprocal_rank = {}\n",
    "        hits_at = {}\n",
    "\n",
    "        for m in missing:\n",
    "            q = examples.clone()\n",
    "            if n_queries > 0:\n",
    "                permutation = torch.randperm(len(examples))[:n_queries]\n",
    "                q = examples[permutation]\n",
    "            if m == 'lhs':\n",
    "                tmp = torch.clone(q[:, 0])\n",
    "                q[:, 0] = q[:, 2]\n",
    "                q[:, 2] = tmp\n",
    "                q[:, 1] += self.n_predicates // 2\n",
    "            ranks = model.get_ranking(q, self.to_skip[m], batch_size=500)\n",
    "            mean_reciprocal_rank[m] = torch.mean(1. / ranks).item()\n",
    "            hits_at[m] = torch.FloatTensor((list(map(\n",
    "                lambda x: torch.mean((ranks <= x).float()).item(),\n",
    "                at\n",
    "            ))))\n",
    "\n",
    "        return mean_reciprocal_rank, hits_at\n",
    "\n",
    "    def time_eval(\n",
    "            self, model: TKBCModel, split: str, n_queries: int = -1, missing_eval: str = 'both',\n",
    "            at: Tuple[int] = (1, 3, 10)\n",
    "    ):\n",
    "        assert missing_eval == 'rhs', \"other evals not implemented\"\n",
    "        test = torch.from_numpy(\n",
    "            self.get_examples(split).astype('int64')\n",
    "        )\n",
    "        if n_queries > 0:\n",
    "            permutation = torch.randperm(len(test))[:n_queries]\n",
    "            test = test[permutation]\n",
    "\n",
    "        time_range = test.float()\n",
    "        sampled_time = (\n",
    "                torch.rand(time_range.shape[0]) * (time_range[:, 4] - time_range[:, 3]) + time_range[:, 3]\n",
    "        ).round().long()\n",
    "        has_end = (time_range[:, 4] != (self.n_timestamps - 1))\n",
    "        has_start = (time_range[:, 3] > 0)\n",
    "\n",
    "        masks = {\n",
    "            'full_time': has_end + has_start,\n",
    "            'only_begin': has_start * (~has_end),\n",
    "            'only_end': has_end * (~has_start),\n",
    "            'no_time': (~has_end) * (~has_start)\n",
    "        }\n",
    "\n",
    "        with_time = torch.cat((\n",
    "            sampled_time.unsqueeze(1),\n",
    "            time_range[:, 0:3].long(),\n",
    "            masks['full_time'].long().unsqueeze(1),\n",
    "            masks['only_begin'].long().unsqueeze(1),\n",
    "            masks['only_end'].long().unsqueeze(1),\n",
    "            masks['no_time'].long().unsqueeze(1),\n",
    "        ), 1)\n",
    "        # generate events\n",
    "        eval_events = sorted(with_time.tolist())\n",
    "\n",
    "        to_filter: Dict[Tuple[int, int], Dict[int, int]] = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        id_event = 0\n",
    "        id_timeline = 0\n",
    "        batch_size = 100\n",
    "        to_filter_batch = []\n",
    "        cur_batch = []\n",
    "\n",
    "        ranks = {\n",
    "            'full_time': [], 'only_begin': [], 'only_end': [], 'no_time': [],\n",
    "            'all': []\n",
    "        }\n",
    "        while id_event < len(eval_events):\n",
    "            # Follow timeline to add events to filters\n",
    "            while id_timeline < len(self.events) and self.events[id_timeline][0] <= eval_events[id_event][3]:\n",
    "                date, event_type, (lhs, rel, rhs) = self.events[id_timeline]\n",
    "                if event_type < 0:  # begin\n",
    "                    to_filter[(lhs, rel)][rhs] += 1\n",
    "                if event_type > 0:  # end\n",
    "                    to_filter[(lhs, rel)][rhs] -= 1\n",
    "                    if to_filter[(lhs, rel)][rhs] == 0:\n",
    "                        del to_filter[(lhs, rel)][rhs]\n",
    "                id_timeline += 1\n",
    "            date, lhs, rel, rhs, full_time, only_begin, only_end, no_time = eval_events[id_event]\n",
    "\n",
    "            to_filter_batch.append(sorted(to_filter[(lhs, rel)].keys()))\n",
    "            cur_batch.append((lhs, rel, rhs, date, full_time, only_begin, only_end, no_time))\n",
    "            # once a batch is ready, call get_ranking and reset\n",
    "            if len(cur_batch) == batch_size or id_event == len(eval_events) - 1:\n",
    "                cuda_batch = torch.cuda.LongTensor(cur_batch)\n",
    "                bbatch = torch.LongTensor(cur_batch)\n",
    "                batch_ranks = model.get_time_ranking(cuda_batch[:, :4], to_filter_batch, 500000)\n",
    "\n",
    "                ranks['full_time'].append(batch_ranks[bbatch[:, 4] == 1])\n",
    "                ranks['only_begin'].append(batch_ranks[bbatch[:, 5] == 1])\n",
    "                ranks['only_end'].append(batch_ranks[bbatch[:, 6] == 1])\n",
    "                ranks['no_time'].append(batch_ranks[bbatch[:, 7] == 1])\n",
    "\n",
    "                ranks['all'].append(batch_ranks)\n",
    "                cur_batch = []\n",
    "                to_filter_batch = []\n",
    "            id_event += 1\n",
    "\n",
    "        ranks = {x: torch.cat(ranks[x]) for x in ranks if len(ranks[x]) > 0}\n",
    "        mean_reciprocal_rank = {x: torch.mean(1. / ranks[x]).item() for x in ranks if len(ranks[x]) > 0}\n",
    "        hits_at = {z: torch.FloatTensor((list(map(\n",
    "            lambda x: torch.mean((ranks[z] <= x).float()).item(),\n",
    "            at\n",
    "        )))) for z in ranks if len(ranks[z]) > 0}\n",
    "\n",
    "        res = {\n",
    "            ('MRR_'+x): y for x, y in mean_reciprocal_rank.items()\n",
    "        }\n",
    "        res.update({('hits@_'+x): y for x, y in hits_at.items()})\n",
    "        return res\n",
    "\n",
    "    def breakdown_time_eval(\n",
    "            self, model: TKBCModel, split: str, n_queries: int = -1, missing_eval: str = 'rhs',\n",
    "    ):\n",
    "        assert missing_eval == 'rhs', \"other evals not implemented\"\n",
    "        test = torch.from_numpy(\n",
    "            self.get_examples(split).astype('int64')\n",
    "        )\n",
    "        if n_queries > 0:\n",
    "            permutation = torch.randperm(len(test))[:n_queries]\n",
    "            test = test[permutation]\n",
    "\n",
    "        time_range = test.float()\n",
    "        sampled_time = (\n",
    "                torch.rand(time_range.shape[0]) * (time_range[:, 4] - time_range[:, 3]) + time_range[:, 3]\n",
    "        ).round().long()\n",
    "        has_end = (time_range[:, 4] != (self.n_timestamps - 1))\n",
    "        has_start = (time_range[:, 3] > 0)\n",
    "\n",
    "        masks = {\n",
    "            'full_time': has_end + has_start,\n",
    "            'only_begin': has_start * (~has_end),\n",
    "            'only_end': has_end * (~has_start),\n",
    "            'no_time': (~has_end) * (~has_start)\n",
    "        }\n",
    "\n",
    "        with_time = torch.cat((\n",
    "            sampled_time.unsqueeze(1),\n",
    "            time_range[:, 0:3].long(),\n",
    "            masks['full_time'].long().unsqueeze(1),\n",
    "            masks['only_begin'].long().unsqueeze(1),\n",
    "            masks['only_end'].long().unsqueeze(1),\n",
    "            masks['no_time'].long().unsqueeze(1),\n",
    "        ), 1)\n",
    "        # generate events\n",
    "        eval_events = sorted(with_time.tolist())\n",
    "\n",
    "        to_filter: Dict[Tuple[int, int], Dict[int, int]] = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        id_event = 0\n",
    "        id_timeline = 0\n",
    "        batch_size = 100\n",
    "        to_filter_batch = []\n",
    "        cur_batch = []\n",
    "\n",
    "        ranks = defaultdict(list)\n",
    "        while id_event < len(eval_events):\n",
    "            # Follow timeline to add events to filters\n",
    "            while id_timeline < len(self.events) and self.events[id_timeline][0] <= eval_events[id_event][3]:\n",
    "                date, event_type, (lhs, rel, rhs) = self.events[id_timeline]\n",
    "                if event_type < 0:  # begin\n",
    "                    to_filter[(lhs, rel)][rhs] += 1\n",
    "                if event_type > 0:  # end\n",
    "                    to_filter[(lhs, rel)][rhs] -= 1\n",
    "                    if to_filter[(lhs, rel)][rhs] == 0:\n",
    "                        del to_filter[(lhs, rel)][rhs]\n",
    "                id_timeline += 1\n",
    "            date, lhs, rel, rhs, full_time, only_begin, only_end, no_time = eval_events[id_event]\n",
    "\n",
    "            to_filter_batch.append(sorted(to_filter[(lhs, rel)].keys()))\n",
    "            cur_batch.append((lhs, rel, rhs, date, full_time, only_begin, only_end, no_time))\n",
    "            # once a batch is ready, call get_ranking and reset\n",
    "            if len(cur_batch) == batch_size or id_event == len(eval_events) - 1:\n",
    "                cuda_batch = torch.cuda.LongTensor(cur_batch)\n",
    "                bbatch = torch.LongTensor(cur_batch)\n",
    "                batch_ranks = model.get_time_ranking(cuda_batch[:, :4], to_filter_batch, 500000)\n",
    "                for rank, predicate in zip(batch_ranks, bbatch[:, 1]):\n",
    "                    ranks[predicate.item()].append(rank.item())\n",
    "                cur_batch = []\n",
    "                to_filter_batch = []\n",
    "            id_event += 1\n",
    "\n",
    "        ranks = {x: torch.FloatTensor(ranks[x]) for x in ranks}\n",
    "        sum_reciprocal_rank = {x: torch.sum(1. / ranks[x]).item() for x in ranks}\n",
    "\n",
    "        return sum_reciprocal_rank\n",
    "\n",
    "    def time_AUC(self, model: TKBCModel, split: str, n_queries: int = -1):\n",
    "        test = torch.from_numpy(\n",
    "            self.get_examples(split).astype('int64')\n",
    "        )\n",
    "        if n_queries > 0:\n",
    "            permutation = torch.randperm(len(test))[:n_queries]\n",
    "            test = test[permutation]\n",
    "\n",
    "        truth, scores = model.get_auc(test.cuda())\n",
    "\n",
    "        return {\n",
    "            'micro': average_precision_score(truth, scores, average='micro'),\n",
    "            'macro': average_precision_score(truth, scores, average='macro')\n",
    "        }\n",
    "\n",
    "    def get_shape(self):\n",
    "        return self.n_entities, self.n_predicates, self.n_entities, self.n_timestamps\n",
    "\n",
    "    def get_original_fact(self, quadruple):\n",
    "        src_id, rel_id, tgt_id, timestamp_id = quadruple\n",
    "\n",
    "        src_name = self.entity_map.get(str(src_id.item()), \"Unknown entity\")\n",
    "        tgt_name = self.entity_map.get(str(tgt_id.item()), \"Unknown entity\")\n",
    "        rel_name = self.rel_map.get(str(rel_id.item()), \"Unknown relation\")\n",
    "        timestamp_value = self.ts_map.get(str(timestamp_id.item()), \"Unknown timestamp\")\n",
    "        \n",
    "        return src_name, rel_name, tgt_name, timestamp_value\n",
    "\n",
    "    def get_original_relation(self, rel_id):\n",
    "        return self.rel_map.get(str(rel_id), \"Unknown relation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0daaca49-e084-47da-9a41-53454beaaeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "class TKBCOptimizer(object):\n",
    "    def __init__(\n",
    "            self, model: TKBCModel,\n",
    "            emb_regularizer: Regularizer, temporal_regularizer: Regularizer,\n",
    "            optimizer: optim.Optimizer, batch_size: int = 256,\n",
    "            verbose: bool = True\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.emb_regularizer = emb_regularizer\n",
    "        self.temporal_regularizer = temporal_regularizer\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def epoch(self, examples: torch.LongTensor):\n",
    "        actual_examples = examples[torch.randperm(examples.shape[0]), :]\n",
    "        loss = nn.CrossEntropyLoss(reduction='mean')\n",
    "        with tqdm.tqdm(total=examples.shape[0], unit='ex', disable=not self.verbose) as bar:\n",
    "            bar.set_description(f'train loss')\n",
    "            b_begin = 0\n",
    "            while b_begin < examples.shape[0]:\n",
    "                input_batch = actual_examples[\n",
    "                    b_begin:b_begin + self.batch_size\n",
    "                ].cuda()\n",
    "                predictions, factors, time = self.model.forward(input_batch)\n",
    "                truth = input_batch[:, 2]\n",
    "\n",
    "                l_fit = loss(predictions, truth)\n",
    "                l_reg = self.emb_regularizer.forward(factors)\n",
    "                l_time = torch.zeros_like(l_reg)\n",
    "                if time is not None:\n",
    "                    l_time = self.temporal_regularizer.forward(time)\n",
    "                l = l_fit + l_reg + l_time\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                l.backward()\n",
    "                self.optimizer.step()\n",
    "                b_begin += self.batch_size\n",
    "                bar.update(input_batch.shape[0])\n",
    "                bar.set_postfix(\n",
    "                    loss=f'{l_fit.item():.0f}',\n",
    "                    reg=f'{l_reg.item():.0f}',\n",
    "                    cont=f'{l_time.item():.0f}'\n",
    "                )\n",
    "\n",
    "\n",
    "class IKBCOptimizer(object):\n",
    "    def __init__(\n",
    "            self, model: TKBCModel,\n",
    "            emb_regularizer: Regularizer, temporal_regularizer: Regularizer,\n",
    "            optimizer: optim.Optimizer, dataset: TemporalDataset, batch_size: int = 256,\n",
    "            verbose: bool = True\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.emb_regularizer = emb_regularizer\n",
    "        self.temporal_regularizer = temporal_regularizer\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def epoch(self, examples: torch.LongTensor):\n",
    "        actual_examples = examples[torch.randperm(examples.shape[0]), :]\n",
    "        loss = nn.CrossEntropyLoss(reduction='mean')\n",
    "        with tqdm.tqdm(total=examples.shape[0], unit='ex', disable=not self.verbose) as bar:\n",
    "            bar.set_description(f'train loss')\n",
    "            b_begin = 0\n",
    "            while b_begin < examples.shape[0]:\n",
    "                time_range = actual_examples[b_begin:b_begin + self.batch_size].cuda()\n",
    "\n",
    "                ## RHS Prediction loss\n",
    "                sampled_time = (\n",
    "                        torch.rand(time_range.shape[0]).cuda() * (time_range[:, 4] - time_range[:, 3]).float() +\n",
    "                        time_range[:, 3].float()\n",
    "                ).round().long()\n",
    "                with_time = torch.cat((time_range[:, 0:3], sampled_time.unsqueeze(1)), 1)\n",
    "\n",
    "                predictions, factors, time = self.model.forward(with_time)\n",
    "                truth = with_time[:, 2]\n",
    "\n",
    "                l_fit = loss(predictions, truth)\n",
    "\n",
    "                ## Time prediction loss (ie cross entropy over time)\n",
    "                time_loss = 0.\n",
    "                if self.model.has_time():\n",
    "                    filtering = ~(\n",
    "                        (time_range[:, 3] == 0) *\n",
    "                        (time_range[:, 4] == (self.dataset.n_timestamps - 1))\n",
    "                    ) # NOT no begin and no end\n",
    "                    these_examples = time_range[filtering, :]\n",
    "                    truth = (\n",
    "                            torch.rand(these_examples.shape[0]).cuda() * (these_examples[:, 4] - these_examples[:, 3]).float() +\n",
    "                            these_examples[:, 3].float()\n",
    "                    ).round().long()\n",
    "                    time_predictions = self.model.forward_over_time(these_examples[:, :3].cuda().long())\n",
    "                    time_loss = loss(time_predictions, truth.cuda())\n",
    "\n",
    "                l_reg = self.emb_regularizer.forward(factors)\n",
    "                l_time = torch.zeros_like(l_reg)\n",
    "                if time is not None:\n",
    "                    l_time = self.temporal_regularizer.forward(time)\n",
    "                l = l_fit + l_reg + l_time + time_loss\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                l.backward()\n",
    "                self.optimizer.step()\n",
    "                b_begin += self.batch_size\n",
    "                bar.update(with_time.shape[0])\n",
    "                bar.set_postfix(\n",
    "                    loss=f'{l_fit.item():.0f}',\n",
    "                    loss_time=f'{time_loss if type(time_loss) == float else time_loss.item() :.0f}',\n",
    "                    reg=f'{l_reg.item():.0f}',\n",
    "                    cont=f'{l_time.item():.4f}'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5b2d156-542c-4e87-a854-7be3020b3588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TComplex Definition\n",
    "class TComplEx(TKBCModel):\n",
    "    def __init__(\n",
    "            self, sizes: Tuple[int, int, int, int], rank: int,\n",
    "            no_time_emb=False, init_size: float = 1e-2\n",
    "    ):\n",
    "        super(TComplEx, self).__init__()\n",
    "        self.sizes = sizes\n",
    "        self.rank = rank\n",
    "\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(s, 2 * rank, sparse=True)\n",
    "            for s in [sizes[0], sizes[1], sizes[3]]\n",
    "        ])\n",
    "        self.embeddings[0].weight.data *= init_size\n",
    "        self.embeddings[1].weight.data *= init_size\n",
    "        self.embeddings[2].weight.data *= init_size\n",
    "\n",
    "        self.no_time_emb = no_time_emb\n",
    "\n",
    "    @staticmethod\n",
    "    def has_time():\n",
    "        return True\n",
    "\n",
    "    def score(self, x):\n",
    "        lhs = self.embeddings[0](x[:, 0])\n",
    "        rel = self.embeddings[1](x[:, 1])\n",
    "        rhs = self.embeddings[0](x[:, 2])\n",
    "        time = self.embeddings[2](x[:, 3])\n",
    "\n",
    "        lhs = lhs[:, :self.rank], lhs[:, self.rank:]\n",
    "        rel = rel[:, :self.rank], rel[:, self.rank:]\n",
    "        rhs = rhs[:, :self.rank], rhs[:, self.rank:]\n",
    "        time = time[:, :self.rank], time[:, self.rank:]\n",
    "\n",
    "        return torch.sum(\n",
    "            (lhs[0] * rel[0] * time[0] - lhs[1] * rel[1] * time[0] -\n",
    "             lhs[1] * rel[0] * time[1] - lhs[0] * rel[1] * time[1]) * rhs[0] +\n",
    "            (lhs[1] * rel[0] * time[0] + lhs[0] * rel[1] * time[0] +\n",
    "             lhs[0] * rel[0] * time[1] - lhs[1] * rel[1] * time[1]) * rhs[1],\n",
    "            1, keepdim=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        lhs = self.embeddings[0](x[:, 0])\n",
    "        rel = self.embeddings[1](x[:, 1])\n",
    "        rhs = self.embeddings[0](x[:, 2])\n",
    "        time = self.embeddings[2](x[:, 3])\n",
    "\n",
    "        lhs = lhs[:, :self.rank], lhs[:, self.rank:]\n",
    "        rel = rel[:, :self.rank], rel[:, self.rank:]\n",
    "        rhs = rhs[:, :self.rank], rhs[:, self.rank:]\n",
    "        time = time[:, :self.rank], time[:, self.rank:]\n",
    "\n",
    "        right = self.embeddings[0].weight\n",
    "        right = right[:, :self.rank], right[:, self.rank:]\n",
    "\n",
    "        rt = rel[0] * time[0], rel[1] * time[0], rel[0] * time[1], rel[1] * time[1]\n",
    "        full_rel = rt[0] - rt[3], rt[1] + rt[2]\n",
    "\n",
    "        return (\n",
    "                       (lhs[0] * full_rel[0] - lhs[1] * full_rel[1]) @ right[0].t() +\n",
    "                       (lhs[1] * full_rel[0] + lhs[0] * full_rel[1]) @ right[1].t()\n",
    "               ), (\n",
    "                   torch.sqrt(lhs[0] ** 2 + lhs[1] ** 2),\n",
    "                   torch.sqrt(full_rel[0] ** 2 + full_rel[1] ** 2),\n",
    "                   torch.sqrt(rhs[0] ** 2 + rhs[1] ** 2)\n",
    "               ), self.embeddings[2].weight[:-1] if self.no_time_emb else self.embeddings[2].weight\n",
    "\n",
    "    def forward_over_time(self, x):\n",
    "        lhs = self.embeddings[0](x[:, 0])\n",
    "        rel = self.embeddings[1](x[:, 1])\n",
    "        rhs = self.embeddings[0](x[:, 2])\n",
    "        time = self.embeddings[2].weight\n",
    "\n",
    "        lhs = lhs[:, :self.rank], lhs[:, self.rank:]\n",
    "        rel = rel[:, :self.rank], rel[:, self.rank:]\n",
    "        rhs = rhs[:, :self.rank], rhs[:, self.rank:]\n",
    "        time = time[:, :self.rank], time[:, self.rank:]\n",
    "\n",
    "        return (\n",
    "                (lhs[0] * rel[0] * rhs[0] - lhs[1] * rel[1] * rhs[0] -\n",
    "                 lhs[1] * rel[0] * rhs[1] + lhs[0] * rel[1] * rhs[1]) @ time[0].t() +\n",
    "                (lhs[1] * rel[0] * rhs[0] - lhs[0] * rel[1] * rhs[0] +\n",
    "                 lhs[0] * rel[0] * rhs[1] - lhs[1] * rel[1] * rhs[1]) @ time[1].t()\n",
    "        )\n",
    "\n",
    "    def get_rhs(self, chunk_begin: int, chunk_size: int):\n",
    "        return self.embeddings[0].weight.data[\n",
    "               chunk_begin:chunk_begin + chunk_size\n",
    "               ].transpose(0, 1)\n",
    "\n",
    "    def get_queries(self, queries: torch.Tensor):\n",
    "        lhs = self.embeddings[0](queries[:, 0])\n",
    "        rel = self.embeddings[1](queries[:, 1])\n",
    "        time = self.embeddings[2](queries[:, 3])\n",
    "        lhs = lhs[:, :self.rank], lhs[:, self.rank:]\n",
    "        rel = rel[:, :self.rank], rel[:, self.rank:]\n",
    "        time = time[:, :self.rank], time[:, self.rank:]\n",
    "        return torch.cat([\n",
    "            lhs[0] * rel[0] * time[0] - lhs[1] * rel[1] * time[0] -\n",
    "            lhs[1] * rel[0] * time[1] - lhs[0] * rel[1] * time[1],\n",
    "            lhs[1] * rel[0] * time[0] + lhs[0] * rel[1] * time[0] +\n",
    "            lhs[0] * rel[0] * time[1] - lhs[1] * rel[1] * time[1]\n",
    "        ], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65a4928d-13a8-4296-ac56-8232a2e23812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume all timestamps are regularly spaced\n",
      "Not using time intervals and events eval\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = pkg_resources.resource_filename('tkbc', 'data/')\n",
    "dataset = TemporalDataset(\"ICEWS14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e43b4018-9847-4f26-8e01-d3f1be206aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 156\n",
    "no_time_emb = False\n",
    "sizes = dataset.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2395b8c-db88-4726-8332-3ab3e4f3fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    # 'ComplEx': ComplEx(sizes, args.rank),\n",
    "    'TComplEx': TComplEx(sizes, rank, no_time_emb=no_time_emb),\n",
    "    # 'TNTComplEx': TNTComplEx(sizes, args.rank, no_time_emb=args.no_time_emb),\n",
    "}['TComplEx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42472ad8-12a6-43a4-add0-fcae5dbc9440",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bc248fc-6004-4e8c-83ed-3a885da7ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adagrad(model.parameters(), lr=1e-1)\n",
    "emb_reg = N3(1e-2)\n",
    "time_reg = Lambda3(1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c70f3b16-adcd-4ee7-8b44-eb49bfa249e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 1000\n",
    "valid_freq = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6415bec-aa33-4c49-82b7-697f8d102f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:08<00:00, 17662.91ex/s, cont=0, loss=4, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 19002.29ex/s, cont=0, loss=2, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:08<00:00, 18039.05ex/s, cont=0, loss=2, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:08<00:00, 16871.14ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:08<00:00, 18012.14ex/s, cont=0, loss=1, reg=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  0.5552375316619873\n",
      "test:  0.542149692773819\n",
      "train:  0.8701666593551636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18848.77ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18249.59ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18621.45ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18529.63ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18339.67ex/s, cont=0, loss=1, reg=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  0.5635833442211151\n",
      "test:  0.5512717366218567\n",
      "train:  0.9130913615226746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18969.87ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18845.27ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18718.85ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 19030.43ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18934.88ex/s, cont=0, loss=1, reg=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  0.5666422247886658\n",
      "test:  0.5573066771030426\n",
      "train:  0.9299018383026123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18598.03ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18329.77ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:08<00:00, 18103.52ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18367.26ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18467.03ex/s, cont=0, loss=1, reg=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  0.5694728493690491\n",
      "test:  0.5605102777481079\n",
      "train:  0.9381696581840515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18454.38ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18757.85ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18610.54ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18554.92ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18550.15ex/s, cont=0, loss=1, reg=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  0.5699145197868347\n",
      "test:  0.5613000690937042\n",
      "train:  0.9443118870258331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18482.79ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18625.02ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18770.64ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18427.17ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18483.34ex/s, cont=0, loss=1, reg=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  0.571316123008728\n",
      "test:  0.5626037418842316\n",
      "train:  0.9483128488063812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18765.22ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18506.90ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18241.35ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18490.52ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18526.27ex/s, cont=0, loss=1, reg=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  0.5728951692581177\n",
      "test:  0.563905656337738\n",
      "train:  0.9519099295139313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:08<00:00, 18130.96ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18509.28ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 18481.31ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 20485.34ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:05<00:00, 24957.79ex/s, cont=0, loss=1, reg=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  0.5725176334381104\n",
      "test:  0.5637607276439667\n",
      "train:  0.9544577300548553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:07<00:00, 19421.36ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:06<00:00, 23602.14ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:05<00:00, 24351.40ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:05<00:00, 24474.18ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:05<00:00, 24481.95ex/s, cont=0, loss=1, reg=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  0.5732938945293427\n",
      "test:  0.5648556351661682\n",
      "train:  0.9568913877010345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:05<00:00, 24905.49ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:05<00:00, 24652.25ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:05<00:00, 24532.46ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:05<00:00, 24700.94ex/s, cont=0, loss=1, reg=1]\n",
      "train loss: 100%|████████████████████████████████████| 145652/145652 [00:06<00:00, 24220.22ex/s, cont=0, loss=1, reg=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  0.5735103487968445\n",
      "test:  0.5642951726913452\n",
      "train:  0.9588110148906708\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    examples = torch.from_numpy(\n",
    "        dataset.get_train().astype('int64')\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    if dataset.has_intervals():\n",
    "        optimizer = IKBCOptimizer(\n",
    "            model, emb_reg, time_reg, opt, dataset,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        optimizer.epoch(examples)\n",
    "\n",
    "    else:\n",
    "        optimizer = TKBCOptimizer(\n",
    "            model, emb_reg, time_reg, opt,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        optimizer.epoch(examples)\n",
    "\n",
    "\n",
    "    def avg_both(mrrs: Dict[str, float], hits: Dict[str, torch.FloatTensor]):\n",
    "        \"\"\"\n",
    "        aggregate metrics for missing lhs and rhs\n",
    "        :param mrrs: d\n",
    "        :param hits:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        m = (mrrs['lhs'] + mrrs['rhs']) / 2.\n",
    "        h = (hits['lhs'] + hits['rhs']) / 2.\n",
    "        return {'MRR': m, 'hits@[1,3,10]': h}\n",
    "\n",
    "    if epoch < 0 or (epoch + 1) % valid_freq == 0:\n",
    "        if dataset.has_intervals():\n",
    "            valid, test, train = [\n",
    "                dataset.eval(model, split, -1 if split != 'train' else 50000)\n",
    "                for split in ['valid', 'test', 'train']\n",
    "            ]\n",
    "            print(\"valid: \", valid)\n",
    "            print(\"test: \", test)\n",
    "            print(\"train: \", train)\n",
    "\n",
    "        else:\n",
    "            valid, test, train = [\n",
    "                avg_both(*dataset.eval(model, split, -1 if split != 'train' else 50000))\n",
    "                for split in ['valid', 'test', 'train']\n",
    "            ]\n",
    "            print(\"valid: \", valid['MRR'])\n",
    "            print(\"test: \", test['MRR'])\n",
    "            print(\"train: \", train['MRR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f035586-ab42-4c06-864a-4419486eda10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if args.model==\"ComplEx\":\n",
    "#     np.save(\"D:\\\\personal-Shreyas\\\\AIRS\\\\notebooks and src\\\\tkbc-main\\\\tkbc-main\\\\tkbc\\\\complex\\\\entity_embeddings.npy\", model.embeddings[0].weight.data.cpu().numpy())\n",
    "#     np.save(\"D:\\\\personal-Shreyas\\\\AIRS\\\\notebooks and src\\\\tkbc-main\\\\tkbc-main\\\\tkbc\\\\complex\\\\relation_embeddings.npy\",model.embeddings[1].weight.data.cpu().numpy())\n",
    "\n",
    "\n",
    "# if args.model==\"TComplEx\":\n",
    "np.save(\"D:\\\\personal-Shreyas\\\\AIRS\\\\model_embeddings\\\\entity_embeddings.npy\", model.embeddings[0].weight.data.cpu().numpy())\n",
    "np.save(\"D:\\\\personal-Shreyas\\\\AIRS\\\\model_embeddings\\\\relation_embeddings.npy\",model.embeddings[1].weight.data.cpu().numpy())\n",
    "np.save(\"D:\\\\personal-Shreyas\\\\AIRS\\\\model_embeddings\\\\time_embeddings.npy\",model.embeddings[2].weight.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "365db033-a340-46fc-983b-1f35c33660ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_correlated_event_triples_with_time_real_only(\n",
    "    head, relation, tail, time, triples, \n",
    "    entity_embeddings_real_np, relation_embeddings_real_np, \n",
    "    time_embeddings_real_np, top_k=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the correlated event triples for a given fact (head, relation, tail, time) using attention weights.\n",
    "    :param head: Tensor containing the head entity\n",
    "    :param relation: Tensor containing the relation\n",
    "    :param tail: Tensor containing the true tail entity (optional for prediction)\n",
    "    :param time: Tensor containing the time information\n",
    "    :param triples: Array of all known triples (head, relation, tail, time)\n",
    "    :param entity_embeddings_real_np: Pre-trained real part of entity embeddings (NumPy array)\n",
    "    :param relation_embeddings_real_np: Pre-trained real part of relation embeddings (NumPy array)\n",
    "    :param time_embeddings_real_np: Pre-trained real part of time embeddings (NumPy array)\n",
    "    :param top_k: Number of top correlated events to return\n",
    "    :return: top_k_event_triples (correlated event triples), correlated_weights (attention weights)\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert embeddings from NumPy to PyTorch tensors\n",
    "    entity_real = torch.tensor(entity_embeddings_real_np)\n",
    "    relation_real = torch.tensor(relation_embeddings_real_np)\n",
    "    time_real = torch.tensor(time_embeddings_real_np)\n",
    "\n",
    "    # Get the real part of the head, relation, tail, and time embeddings for the query\n",
    "    head_real = entity_real[head]\n",
    "    relation_real = relation_real[relation]\n",
    "    tail_real = entity_real[tail]\n",
    "    time_real_query = time_real[time]\n",
    "\n",
    "    # Initialize lists to store correlated triples and their attention weights\n",
    "    correlated_triples = []\n",
    "    correlated_weights = []\n",
    "\n",
    "    # Loop through the known triples and compute the correlation with the query triple\n",
    "    for i, (h, r, t, time_idx) in enumerate(triples):\n",
    "        # Get embeddings for the current triple in the dataset\n",
    "        h_real = entity_real[h]\n",
    "        r_real = relation_real[r]\n",
    "        t_real = entity_real[t]\n",
    "        time_real_db = time_real[time_idx]\n",
    "\n",
    "        # Compute the score using only the real parts, including time embeddings\n",
    "        # ComplEx scoring (simplified to only the real part with time embeddings)\n",
    "        score_query = torch.sum(head_real * relation_real * tail_real * time_real_query)\n",
    "        score_db = torch.sum(h_real * r_real * t_real * time_real_db)\n",
    "\n",
    "        # Combine the scores for correlation\n",
    "        combined_score = score_query * score_db\n",
    "\n",
    "        # Add the triple and the combined score to the list\n",
    "        correlated_triples.append((h, r, t, time_idx))\n",
    "        correlated_weights.append(combined_score)\n",
    "\n",
    "    # Convert the list of scores to a tensor\n",
    "    correlated_weights = torch.stack(correlated_weights)\n",
    "\n",
    "    # Get the top-K triples with the highest combined attention weights\n",
    "    top_k_weights, top_k_indices = torch.topk(correlated_weights, k=top_k)\n",
    "    top_k_triples = [correlated_triples[idx] for idx in top_k_indices]\n",
    "\n",
    "    # Print the attention weight for the true tail entity\n",
    "    print(f\"Attention weight for true tail entity ({tail.item()}): {correlated_weights[tail].item()}\")\n",
    "\n",
    "    # Return the top-k triples and their correlation scores\n",
    "    return top_k_triples, top_k_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1393616-5a17-40fc-8968-c4c06f08b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example NumPy arrays for embeddings\n",
    "entity_embeddings_real_np = np.load('D:\\\\personal-Shreyas\\\\AIRS\\\\model_embeddings\\\\entity_embeddings.npy')\n",
    "relation_embeddings_real_np = np.load('D:\\\\personal-Shreyas\\\\AIRS\\\\model_embeddings\\\\relation_embeddings.npy')\n",
    "time_embeddings_real_np = np.load('D:\\\\personal-Shreyas\\\\AIRS\\\\model_embeddings\\\\time_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c340884-384b-4834-9658-d5a09553b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = torch.tensor([132])\n",
    "relation = torch.tensor([9])\n",
    "tail = torch.tensor([1])\n",
    "time = torch.tensor([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b96fd12a-b2dc-45c8-88ed-a2601a5cc68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the triples from the dataset\n",
    "train_triples_np = dataset.get_train()\n",
    "train_triples = [tuple(triple) for triple in train_triples_np]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33901094-0acf-4be0-96a2-53af1a3ce523",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 56 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m top_k_triples, top_k_scores \u001b[38;5;241m=\u001b[39m \u001b[43mget_correlated_event_triples_with_time_real_only\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_triples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity_embeddings_real_np\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelation_embeddings_real_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_embeddings_real_np\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 41\u001b[0m, in \u001b[0;36mget_correlated_event_triples_with_time_real_only\u001b[1;34m(head, relation, tail, time, triples, entity_embeddings_real_np, relation_embeddings_real_np, time_embeddings_real_np, top_k)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (h, r, t, time_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(triples):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Get embeddings for the current triple in the dataset\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     h_real \u001b[38;5;241m=\u001b[39m entity_real[h]\n\u001b[1;32m---> 41\u001b[0m     r_real \u001b[38;5;241m=\u001b[39m \u001b[43mrelation_real\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     42\u001b[0m     t_real \u001b[38;5;241m=\u001b[39m entity_real[t]\n\u001b[0;32m     43\u001b[0m     time_real_db \u001b[38;5;241m=\u001b[39m time_real[time_idx]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 56 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "top_k_triples, top_k_scores = get_correlated_event_triples_with_time_real_only(\n",
    "    head, relation, tail, time,\n",
    "    train_triples, entity_embeddings_real_np,\n",
    "    relation_embeddings_real_np, time_embeddings_real_np,\n",
    "    top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e45a7d-6a51-48e6-877b-46bae476d88c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
