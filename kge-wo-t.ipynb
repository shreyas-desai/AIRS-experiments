{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac2ccbb-357d-485d-8ebd-ca66dec79fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Tuple, List, Dict\n",
    "from pathlib import Path\n",
    "import pkg_resources\n",
    "import pickle\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from typing import Dict\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dce3376-306f-48af-a9d2-0a643dd7e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularizers\n",
    "class Regularizer(nn.Module, ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, factors: Tuple[torch.Tensor]):\n",
    "        pass\n",
    "\n",
    "class N3(Regularizer):\n",
    "    def __init__(self, weight: float):\n",
    "        super(N3, self).__init__()\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, factors):\n",
    "        norm = 0\n",
    "        for f in factors:\n",
    "            norm += self.weight * torch.sum(torch.abs(f) ** 3)\n",
    "        return norm / factors[0].shape[0]\n",
    "\n",
    "\n",
    "class Lambda3(Regularizer):\n",
    "    def __init__(self, weight: float):\n",
    "        super(Lambda3, self).__init__()\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, factor):\n",
    "        ddiff = factor[1:] - factor[:-1]\n",
    "        rank = int(ddiff.shape[1] / 2)\n",
    "        diff = torch.sqrt(ddiff[:, :rank]**2 + ddiff[:, rank:]**2)**3\n",
    "        return self.weight * torch.sum(diff) / (factor.shape[0] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ab10cba-8398-4fd4-bd3a-8cdbbff23ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TKBCModel(nn.Module, ABC):\n",
    "    @abstractmethod\n",
    "    def get_rhs(self, chunk_begin: int, chunk_size: int):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_queries(self, queries: torch.Tensor):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def score(self, x: torch.Tensor):\n",
    "        pass\n",
    "\n",
    "    def get_ranking(\n",
    "            self, queries: torch.Tensor,\n",
    "            filters: Dict[Tuple[int, int], List[int]],\n",
    "            batch_size: int = 1000, chunk_size: int = -1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns filtered ranking for each query (lhs, rel, rhs).\n",
    "        :param queries: a torch.LongTensor of triples (lhs, rel, rhs)\n",
    "        :param filters: filters[(lhs, rel)] gives the elements to filter from ranking\n",
    "        :param batch_size: maximum number of queries processed at once\n",
    "        :param chunk_size: maximum number of candidates processed at once\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if chunk_size < 0:\n",
    "            chunk_size = self.sizes[2]\n",
    "        ranks = torch.ones(len(queries))\n",
    "        with torch.no_grad():\n",
    "            c_begin = 0\n",
    "            while c_begin < self.sizes[2]:\n",
    "                b_begin = 0\n",
    "                rhs = self.get_rhs(c_begin, chunk_size)\n",
    "                while b_begin < len(queries):\n",
    "                    these_queries = queries[b_begin:b_begin + batch_size]\n",
    "                    q = self.get_queries(these_queries)\n",
    "\n",
    "                    scores = q @ rhs\n",
    "                    targets = self.score(these_queries)\n",
    "                    assert not torch.any(torch.isinf(scores)), \"inf scores\"\n",
    "                    assert not torch.any(torch.isnan(scores)), \"nan scores\"\n",
    "                    assert not torch.any(torch.isinf(targets)), \"inf targets\"\n",
    "                    assert not torch.any(torch.isnan(targets)), \"nan targets\"\n",
    "\n",
    "                    # set filtered and true scores to -1e6 to be ignored\n",
    "                    # take care that scores are chunked\n",
    "                    for i, query in enumerate(these_queries):\n",
    "                        filter_out = filters[(query[0].item(), query[1].item())]\n",
    "                        filter_out += [queries[b_begin + i, 2].item()]\n",
    "                        if chunk_size < self.sizes[2]:\n",
    "                            filter_in_chunk = [\n",
    "                                int(x - c_begin) for x in filter_out\n",
    "                                if c_begin <= x < c_begin + chunk_size\n",
    "                            ]\n",
    "                            scores[i, torch.LongTensor(filter_in_chunk)] = -1e6\n",
    "                        else:\n",
    "                            scores[i, torch.LongTensor(filter_out)] = -1e6\n",
    "                    ranks[b_begin:b_begin + batch_size] += torch.sum(\n",
    "                        (scores >= targets).float(), dim=1\n",
    "                    ).cpu()\n",
    "\n",
    "                    b_begin += batch_size\n",
    "\n",
    "                c_begin += chunk_size\n",
    "        return ranks\n",
    "\n",
    "    def get_auc(\n",
    "            self, queries: torch.Tensor, batch_size: int = 1000\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns filtered ranking for each query.\n",
    "        :param queries: a torch.LongTensor of triples (lhs, rel, rhs)\n",
    "        :param batch_size: maximum number of queries processed at once\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        all_scores, all_truth = [], []\n",
    "        with torch.no_grad():\n",
    "            b_begin = 0\n",
    "            while b_begin < len(queries):\n",
    "                these_queries = queries[b_begin:b_begin + batch_size]\n",
    "                scores = self.score(these_queries)\n",
    "                assert not torch.any(torch.isinf(scores) + torch.isnan(scores)), \"inf or nan scores\"\n",
    "                all_scores.append(scores.cpu().numpy())\n",
    "                truth = torch.ones_like(scores).cpu().numpy()  # Assuming ground truth as 1 for correct triples\n",
    "                all_truth.append(truth)\n",
    "                b_begin += batch_size\n",
    "\n",
    "        return np.concatenate(all_truth), np.concatenate(all_scores)\n",
    "\n",
    "    def get_time_ranking(\n",
    "            self, queries: torch.Tensor, filters: List[List[int]], chunk_size: int = -1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns filtered ranking for a batch of queries.\n",
    "        :param queries: a torch.LongTensor of triples (lhs, rel, rhs)\n",
    "        :param filters: ordered filters\n",
    "        :param chunk_size: maximum number of candidates processed at once\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if chunk_size < 0:\n",
    "            chunk_size = self.sizes[2]\n",
    "        ranks = torch.ones(len(queries))\n",
    "        with torch.no_grad():\n",
    "            c_begin = 0\n",
    "            q = self.get_queries(queries)\n",
    "            targets = self.score(queries)\n",
    "            while c_begin < self.sizes[2]:\n",
    "                rhs = self.get_rhs(c_begin, chunk_size)\n",
    "                scores = q @ rhs\n",
    "                # set filtered and true scores to -1e6 to be ignored\n",
    "                # take care that scores are chunked\n",
    "                for i, (query, filter) in enumerate(zip(queries, filters)):\n",
    "                    filter_out = filter + [query[2].item()]\n",
    "                    if chunk_size < self.sizes[2]:\n",
    "                        filter_in_chunk = [\n",
    "                            int(x - c_begin) for x in filter_out\n",
    "                            if c_begin <= x < c_begin + chunk_size\n",
    "                        ]\n",
    "                        scores[i, filter_in_chunk] = -1e6\n",
    "                    else:\n",
    "                        scores[i, filter_out] = -1e6\n",
    "                ranks += torch.sum(\n",
    "                    (scores >= targets).float(), dim=1\n",
    "                ).cpu()\n",
    "\n",
    "                c_begin += chunk_size\n",
    "        return ranks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83f78bd8-6651-4484-a75b-d56e4e67d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, name: str):\n",
    "        self.root = Path(DATA_PATH) / name\n",
    "        self.entity_map = {}\n",
    "        self.rel_map = {}\n",
    "\n",
    "        # Load entity and relation mappings (no timestamps)\n",
    "        with open(self.root / \"ent_id\", 'r', encoding=\"utf-8\") as file:\n",
    "            for row in file.readlines():\n",
    "                ent, code = row.split('\\t')\n",
    "                self.entity_map[code.strip()] = ent\n",
    "                \n",
    "        with open(self.root / \"rel_id\", 'r', encoding=\"utf-8\") as file:\n",
    "            for row in file.readlines():\n",
    "                rel, code = row.split('\\t')\n",
    "                self.rel_map[code.strip()] = rel\n",
    "\n",
    "        # Load the dataset: train, test, and validation\n",
    "        self.data = {}\n",
    "        for split in ['train', 'test', 'valid']:\n",
    "            in_file = open(str(self.root / (split + '.pickle')), 'rb')\n",
    "            self.data[split] = pickle.load(in_file)\n",
    "\n",
    "        # Get the number of entities and predicates (relations)\n",
    "        maxis = np.max(self.data['train'], axis=0)\n",
    "        self.n_entities = int(max(maxis[0], maxis[2]) + 1)  # head or tail\n",
    "        self.n_predicates = int(maxis[1] + 1)  # relations\n",
    "\n",
    "    def get_examples(self, split: str):\n",
    "        \"\"\"Get triples for a given split.\"\"\"\n",
    "        return self.data[split]\n",
    "\n",
    "    def get_train(self):\n",
    "        \"\"\"Prepare training triples, including inverse relations.\"\"\"\n",
    "        copy = np.copy(self.data['train'])\n",
    "        tmp = np.copy(copy[:, 0])\n",
    "        copy[:, 0] = copy[:, 2]\n",
    "        copy[:, 2] = tmp\n",
    "        copy[:, 1] += self.n_predicates  # inverse relation\n",
    "        return np.vstack((self.data['train'], copy))\n",
    "\n",
    "    def eval(self, model, split: str, n_queries: int = -1, missing_eval: str = 'both', at: Tuple[int] = (1, 3, 10)):\n",
    "        \"\"\"Evaluate the model on a split using triples without temporal fields.\"\"\"\n",
    "        test = self.get_examples(split)\n",
    "        examples = torch.from_numpy(test.astype('int64')).cuda()\n",
    "\n",
    "        missing = [missing_eval] if missing_eval != 'both' else ['rhs', 'lhs']\n",
    "        mean_reciprocal_rank = {}\n",
    "        hits_at = {}\n",
    "\n",
    "        for m in missing:\n",
    "            q = examples.clone()\n",
    "            if n_queries > 0:\n",
    "                permutation = torch.randperm(len(examples))[:n_queries]\n",
    "                q = examples[permutation]\n",
    "\n",
    "            if m == 'lhs':\n",
    "                tmp = q[:, 0].clone()\n",
    "                q[:, 0] = q[:, 2]\n",
    "                q[:, 2] = tmp\n",
    "                q[:, 1] += self.n_predicates  # handle inverse relations\n",
    "\n",
    "            ranks = model.get_ranking(q, batch_size=500)  # Rank predictions based on the model\n",
    "            mean_reciprocal_rank[m] = torch.mean(1. / ranks).item()\n",
    "            hits_at[m] = torch.FloatTensor([torch.mean((ranks <= x).float()).item() for x in at])\n",
    "\n",
    "        return mean_reciprocal_rank, hits_at\n",
    "\n",
    "    def get_shape(self):\n",
    "        \"\"\"Return the number of entities and relations (no timestamps).\"\"\"\n",
    "        return self.n_entities, self.n_predicates, self.n_entities\n",
    "\n",
    "    def get_original_fact(self, triple):\n",
    "        \"\"\"Return the textual form of a triple (no timestamps).\"\"\"\n",
    "        src_id, rel_id, tgt_id = triple\n",
    "        src_name = self.entity_map.get(str(src_id.item()), \"Unknown entity\")\n",
    "        tgt_name = self.entity_map.get(str(tgt_id.item()), \"Unknown entity\")\n",
    "        rel_name = self.rel_map.get(str(rel_id.item()), \"Unknown relation\")\n",
    "        return src_name, rel_name, tgt_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98dfd46e-ed2d-46dd-9c5e-c271b1bc2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TKBCOptimizer(object):\n",
    "    def __init__(\n",
    "            self, model: TKBCModel,\n",
    "            emb_regularizer: Regularizer,\n",
    "            optimizer: optim.Optimizer, batch_size: int = 256,\n",
    "            verbose: bool = True\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.emb_regularizer = emb_regularizer\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def epoch(self, examples: torch.LongTensor):\n",
    "        \"\"\"\n",
    "        Perform a single epoch of training on the provided examples.\n",
    "        :param examples: a torch.LongTensor of triples (lhs, rel, rhs)\n",
    "        \"\"\"\n",
    "        actual_examples = examples[torch.randperm(examples.shape[0]), :]\n",
    "        loss = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "        with tqdm.tqdm(total=examples.shape[0], unit='ex', disable=not self.verbose) as bar:\n",
    "            bar.set_description(f'train loss')\n",
    "            b_begin = 0\n",
    "            while b_begin < examples.shape[0]:\n",
    "                input_batch = actual_examples[\n",
    "                    b_begin:b_begin + self.batch_size\n",
    "                ].cuda()\n",
    "                \n",
    "                # Forward pass of the model for static triples (no temporal component)\n",
    "                predictions, factors = self.model.forward(input_batch)\n",
    "                truth = input_batch[:, 2]  # The ground truth rhs for the triples\n",
    "\n",
    "                # Compute the loss\n",
    "                l_fit = loss(predictions, truth)\n",
    "                l_reg = self.emb_regularizer.forward(factors)\n",
    "                l = l_fit + l_reg\n",
    "\n",
    "                # Backpropagation and optimizer step\n",
    "                self.optimizer.zero_grad()\n",
    "                l.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                b_begin += self.batch_size\n",
    "                bar.update(input_batch.shape[0])\n",
    "                bar.set_postfix(\n",
    "                    loss=f'{l_fit.item():.0f}',\n",
    "                    reg=f'{l_reg.item():.0f}'\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcf464f9-58d9-4790-9e6a-bff0fd9f5f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ComplEx(TKBCModel):\n",
    "    def __init__(self, sizes: Tuple[int, int, int], rank: int, init_size: float = 1e-2):\n",
    "        super(ComplEx, self).__init__()\n",
    "        self.sizes = sizes  # (num_entities, num_relations)\n",
    "        self.rank = rank\n",
    "\n",
    "        # Embedding for entities and relations, real and imaginary parts\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(s, 2 * rank, sparse=True) for s in sizes\n",
    "        ])\n",
    "\n",
    "        # Initialize weights\n",
    "        for emb in self.embeddings:\n",
    "            emb.weight.data *= init_size\n",
    "\n",
    "    def score(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Scoring function for triples (lhs, rel, rhs) using ComplEx model.\n",
    "        :param x: torch.Tensor of shape (batch_size, 3) containing (lhs, rel, rhs)\n",
    "        :return: ComplEx score for each triple\n",
    "        \"\"\"\n",
    "        lhs = self.embeddings[0](x[:, 0])  # Left-hand-side entity (head)\n",
    "        rel = self.embeddings[1](x[:, 1])  # Relation\n",
    "        rhs = self.embeddings[0](x[:, 2])  # Right-hand-side entity (tail)\n",
    "\n",
    "        # Split real and imaginary parts\n",
    "        lhs_real, lhs_imag = lhs[:, :self.rank], lhs[:, self.rank:]\n",
    "        rel_real, rel_imag = rel[:, :self.rank], rel[:, self.rank:]\n",
    "        rhs_real, rhs_imag = rhs[:, :self.rank], rhs[:, self.rank:]\n",
    "\n",
    "        # ComplEx scoring function (real and imaginary parts)\n",
    "        score_real = lhs_real * rel_real * rhs_real + lhs_imag * rel_imag * rhs_imag\n",
    "        score_imag = lhs_real * rel_imag * rhs_imag - lhs_imag * rel_real * rhs_real\n",
    "\n",
    "        return torch.sum(score_real + score_imag, dim=1, keepdim=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward function for (lhs, rel, rhs) triples.\n",
    "        :param x: torch.Tensor of shape (batch_size, 3) containing (lhs, rel, rhs)\n",
    "        :return: Scores for all rhs entities (used for ranking)\n",
    "        \"\"\"\n",
    "        lhs = self.embeddings[0](x[:, 0])  # Head\n",
    "        rel = self.embeddings[1](x[:, 1])  # Relation\n",
    "\n",
    "        lhs_real, lhs_imag = lhs[:, :self.rank], lhs[:, self.rank:]\n",
    "        rel_real, rel_imag = rel[:, :self.rank], rel[:, self.rank:]\n",
    "\n",
    "        rhs = self.embeddings[0].weight  # All right-hand-side entities\n",
    "        rhs_real, rhs_imag = rhs[:, :self.rank], rhs[:, self.rank:]\n",
    "\n",
    "        return (\n",
    "            (lhs_real * rel_real - lhs_imag * rel_imag) @ rhs_real.T + \n",
    "            (lhs_real * rel_imag + lhs_imag * rel_real) @ rhs_imag.T\n",
    "        ), (\n",
    "            torch.sqrt(lhs_real**2 + lhs_imag**2),\n",
    "            torch.sqrt(rel_real**2 + rel_imag**2),\n",
    "            torch.sqrt(rhs_real**2 + rhs_imag**2)\n",
    "        )\n",
    "\n",
    "    def get_rhs(self, chunk_begin: int, chunk_size: int):\n",
    "        \"\"\"\n",
    "        Get the right-hand side (tail) embeddings for a chunk.\n",
    "        \"\"\"\n",
    "        return self.embeddings[0].weight.data[\n",
    "               chunk_begin:chunk_begin + chunk_size\n",
    "               ].transpose(0, 1)\n",
    "\n",
    "    def get_queries(self, queries: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Generate query embeddings for (lhs, rel) pairs.\n",
    "        \"\"\"\n",
    "        lhs = self.embeddings[0](queries[:, 0])  # Head\n",
    "        rel = self.embeddings[1](queries[:, 1])  # Relation\n",
    "\n",
    "        lhs_real, lhs_imag = lhs[:, :self.rank], lhs[:, self.rank:]\n",
    "        rel_real, rel_imag = rel[:, :self.rank], rel[:, self.rank:]\n",
    "\n",
    "        return torch.cat([\n",
    "            lhs_real * rel_real - lhs_imag * rel_imag,\n",
    "            lhs_real * rel_imag + lhs_imag * rel_real\n",
    "        ], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f50f686-0464-4386-8d60-d27967d7ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = pkg_resources.resource_filename('tkbc', 'data/')\n",
    "dataset = Dataset(\"ICEWS14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce8601a7-44a6-4f3a-ae0a-5a7349cb6b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 156\n",
    "no_time_emb = True\n",
    "sizes = dataset.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a6f826d-a49a-424e-9997-059fb79a1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    # 'ComplEx': ComplEx(sizes, args.rank),\n",
    "    'ComplEx': ComplEx(sizes, rank),\n",
    "    # 'TNTComplEx': TNTComplEx(sizes, args.rank, no_time_emb=args.no_time_emb),\n",
    "}['ComplEx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a057ca2-1f95-4de9-b923-233bb4375741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "115511d1-0f94-4ccf-90cb-93acf9089607",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adagrad(model.parameters(), lr=1e-1)\n",
    "emb_reg = N3(1e-2)\n",
    "time_reg = Lambda3(1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2d57168-6751-475f-a723-645773e85773",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 1000\n",
    "valid_freq = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc28fda6-9062-411a-9f2f-a803a91462ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss:   0%|                                                                           | 0/145652 [00:00<?, ?ex/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Use only the non-temporal optimizer (TKBCOptimizer)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m TKBCOptimizer(\n\u001b[0;32m     10\u001b[0m     model, emb_reg, opt,  \u001b[38;5;66;03m# Removed time_reg\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size\n\u001b[0;32m     12\u001b[0m )\n\u001b[1;32m---> 13\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Train on the examples\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mavg_both\u001b[39m(mrrs: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], hits: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mFloatTensor]):\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    Aggregate metrics for missing lhs and rhs.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    :param mrrs: Dictionary containing MRRs for lhs and rhs\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    :param hits: Dictionary containing hit scores for lhs and rhs\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    :return: Aggregated metrics\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 31\u001b[0m, in \u001b[0;36mTKBCOptimizer.epoch\u001b[1;34m(self, examples)\u001b[0m\n\u001b[0;32m     26\u001b[0m input_batch \u001b[38;5;241m=\u001b[39m actual_examples[\n\u001b[0;32m     27\u001b[0m     b_begin:b_begin \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m     28\u001b[0m ]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Forward pass of the model for static triples (no temporal component)\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m predictions, factors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m truth \u001b[38;5;241m=\u001b[39m input_batch[:, \u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# The ground truth rhs for the triples\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 47\u001b[0m, in \u001b[0;36mComplEx.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     42\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    Forward function for (lhs, rel, rhs) triples.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    :param x: torch.Tensor of shape (batch_size, 3) containing (lhs, rel, rhs)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m    :return: Scores for all rhs entities (used for ranking)\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     lhs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Head\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     rel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings[\u001b[38;5;241m1\u001b[39m](x[:, \u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Relation\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     lhs_real, lhs_imag \u001b[38;5;241m=\u001b[39m lhs[:, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank], lhs[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank:]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\nn\\functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    # Convert training dataset to PyTorch LongTensor\n",
    "    examples = torch.from_numpy(dataset.get_train().astype('int64'))\n",
    "\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Use only the non-temporal optimizer (TKBCOptimizer)\n",
    "    optimizer = TKBCOptimizer(\n",
    "        model, emb_reg, opt,  # Removed time_reg\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    optimizer.epoch(examples)  # Train on the examples\n",
    "\n",
    "    def avg_both(mrrs: Dict[str, float], hits: Dict[str, torch.FloatTensor]):\n",
    "        \"\"\"\n",
    "        Aggregate metrics for missing lhs and rhs.\n",
    "        :param mrrs: Dictionary containing MRRs for lhs and rhs\n",
    "        :param hits: Dictionary containing hit scores for lhs and rhs\n",
    "        :return: Aggregated metrics\n",
    "        \"\"\"\n",
    "        m = (mrrs['lhs'] + mrrs['rhs']) / 2.\n",
    "        h = (hits['lhs'] + hits['rhs']) / 2.\n",
    "        return {'MRR': m, 'hits@[1,3,10]': h}\n",
    "\n",
    "    # Validation and logging every `valid_freq` epochs\n",
    "    if epoch < 0 or (epoch + 1) % valid_freq == 0:\n",
    "        # Evaluate on valid, test, and train sets\n",
    "        valid, test, train = [\n",
    "            avg_both(*dataset.eval(model, split, -1 if split != 'train' else 50000))\n",
    "            for split in ['valid', 'test', 'train']\n",
    "        ]\n",
    "\n",
    "        # Print evaluation metrics\n",
    "        print(\"valid: \", valid['MRR'])\n",
    "        print(\"test: \", test['MRR'])\n",
    "        print(\"train: \", train['MRR'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09225b16-0832-4978-921d-1ff85e3aa754",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
