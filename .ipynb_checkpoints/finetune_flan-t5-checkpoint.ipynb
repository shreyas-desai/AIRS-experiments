{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8783270,"sourceType":"datasetVersion","datasetId":5279914}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom sklearn.model_selection import train_test_split","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/rawdata/rawdat/AFG/quadruple.txt\", sep='\\t', names = [\"source\",\"relation\",\"target\",\"date\"])\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data = pd.read_csv(\"../data/raw_data/rawdat/AFG/quadruple.txt\", sep='\\t', names = [\"source\",\"relation\",\"target\",\"date\"])\n# data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text =pd.read_csv(\"/kaggle/input/rawdata/rawdat/AFG/text.txt\", sep='\\t', names = [\"text\"])\ntext.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['source'] = data['source'].str.strip()\ndata['relation'] = data['relation'].str.strip()\ndata['target'] = data['target'].str.strip()\ntext['text'] = text['text'].str.strip()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = pd.concat([data,text],axis=1)\nresult.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result['text'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_result = result.drop_duplicates()\n# unique_result = result.drop_duplicates(subset=['text'],keep='last')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(unique_result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# groupby_df = pd.DataFrame(unique_result.groupby([\"source\",\"date\"])[['target','text','relation']].agg(lambda x: '///'.join(x)).reset_index())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(groupby_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# groupby_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in range(200):\n#     print(groupby_df['source'][i],\"\\t\",groupby_df['relation'][i],\"\\t\",groupby_df['target'][i],\"\\n\", groupby_df['text'][i],\"\\n\")","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tried In-context learning, gives answers close to what we want, but hallucinates sometimes. There is a limit to number of input tokens that can be provided.","metadata":{}},{"cell_type":"code","source":"# print(f\"Unique Sources: {len(result['source'].unique())} \\nUnique Targets: {len(result['target'].unique())}\\nUnique Relations: {len(result['relation'].unique())} \\nUnique Texts: {len(result['text'].unique())}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_text = \"Given an event text, extract the source, target and relation between them: For e.g.\\n\"\n# # for t in range(len(result)):\n# for t in range(len(result)):\n#     if len(input_text.split())>=512:\n#         input_text += f\"Given Text: {result['text'][t]}\"\n#         print(f\"Prediction text :{result['text'][t]}\\n Extracted information -> Source: {result['source'][t]}, Target: {result['target'][t]}, Relation: {result['relation'][t]}\\n\")\n#         break\n#     input_text += f\"Given text:{result['text'][t]}\\n Extracted information -> Source:{result['source'][t]}, Target:{result['target'][t]}, Relation:{result['relation'][t]}\\n\"\n\n# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# outputs = model.generate(input_ids,max_new_tokens=1000)\n# print(tokenizer.decode(outputs[0],skip_special_tokens=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tried Chain-of-Thought Prompting. Didn't work well as the reasoning didn't make much sense.","metadata":{}},{"cell_type":"code","source":"# cot_input = f\"Given a text: 'In western Farah province, which like Badghis is experiencing escalating violence as Taliban influence spreads to previously peaceful areas, three militants were killed by police in a shootout late Thursday, police said', we extract the source:'Police (Afghanistan)', target:'Police (Afghanistan)' and relation:' Use conventional military force'. Explain the reasoning as to how the source, target and relation are extracted based on the text.\"\n# cot_input_ids = tokenizer(cot_input, return_tensors=\"pt\").input_ids.to(\"cuda\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cot_outputs = model.generate(cot_input_ids,max_new_tokens=1000)\n# print(tokenizer.decode(cot_outputs[0],skip_special_tokens=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training a supervised model using FLAN-T5 for multiclass classification of event type","metadata":{}},{"cell_type":"code","source":"# def generate_training_data(df: pd.DataFrame, ids: np.ndarray, masks: np.ndarray, tokenizer):\n#     for i,text in tqdm(enumerate(df['text'])):\n#         tokens = tokenizer.encode_plus(\n#             text,\n#             max_length = 256,\n#             truncation = True,\n#             padding = 'max_length',\n#             add_special_tokens = True,\n#             return_tensors = 'pt'          \n#         )\n#         ids[i, :] = tokens.input_ids\n#         masks[i, :] = tokens.attention_mask\n#     return ids, masks        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_input_ids = np.zeros((len(unique_result),256))\n# X_attn_masks = np.zeros((len(unique_result),256))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_input_ids, X_attn_masks = generate_training_data(unique_result, X_input_ids, X_attn_masks, tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(unique_result['relation'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"relation_ids = pd.read_csv('/kaggle/input/rawdata/rawdat/AFG/relation2id.txt', sep='\\t', names=['relation','id'])\nrelation_ids.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"relation_id_maps = pd.Series(relation_ids.id.values, index=relation_ids.relation).to_dict()\nrelation_id_maps","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_result['relation_to_id'] = unique_result['relation'].map(relation_id_maps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_result.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_input_ids","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# labels = np.zeros((len(unique_result), len(relation_ids)))\n# labels.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# labels[np.arange(len(unique_result)), unique_result['relation_to_id'].values] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = F.one_hot(torch.randint(0, 10, (10,)),num_classes = 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = torch.randint(0, 10, (10,))\n\n# labels --> one-hot \none_hot = torch.nn.functional.one_hot(labels)\n# one-hot --> labels\nlabels_again = torch.argmax(one_hot, dim=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_ids = y[:, :-1].contiguous()\nlm_labels = y[:, 1:].clone().detach()\nlm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_ids","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class EventDataset(Dataset):\n#     def __init__(self, dataframe, tokenizer, source_len, source_text, target_label):\n#         self.tokenizer = tokenizer\n#         self.data = dataframe\n#         self.source_len = source_len\n# #         self.target_len = target_len\n#         self.source_text = dataframe[source_text]\n#         self.target_label = dataframe[target_label]\n        \n#     def __len__(self):\n#         return len(self.data)\n    \n#     def __getitem__(self,index):\n#         source_text = str(self.source_text[index])\n#         target_label = int(self.target_label[index])\n#         target_label = F.one_hot(torch.tensor(target_label, dtype=torch.long),num_classes = len(relation_ids))\n        \n#         source = self.tokenizer.batch_encode_plus(\n#             [source_text],\n#             max_length=self.source_len,\n#             pad_to_max_length=True,\n#             truncation=True,\n#             padding=\"max_length\",\n#             return_tensors=\"pt\",\n#         )\n# #         target = self.tokenizer.batch_encode_plus(\n# #             [target_text],\n# #             max_length=self.target_len,\n# #             pad_to_max_length=True,\n# #             truncation=True,\n# #             padding=\"max_length\",\n# #             return_tensors=\"pt\",\n# #         )\n\n#         source_ids = source[\"input_ids\"].squeeze()\n#         source_mask = source[\"attention_mask\"].squeeze()\n# #         target_ids = target[\"input_ids\"].squeeze()\n# #         target_mask = target[\"attention_mask\"].squeeze()\n\n#         return {\n#             \"source_ids\": source_ids.to(dtype=torch.long),\n#             \"source_mask\": source_mask.to(dtype=torch.long),\n# #             \"target_ids\": target_ids.to(dtype=torch.long),\n# #             \"target_ids_y\": target_ids.to(dtype=torch.long),\n#             \"labels\": target_label,\n#         }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EventDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, source_len, source_text, target_label):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.source_len = source_len\n        self.source_text = dataframe[source_text]\n        self.target_label = dataframe[target_label]\n\n    def __len__(self):\n        return len(self.target_label)\n    \n    def __getitem__(self, index):\n        source_text = str(self.source_text[index])\n        target_label = int(self.target_label[index])\n        \n        source = self.tokenizer.batch_encode_plus(\n            [source_text],\n            max_length=self.source_len,\n            pad_to_max_length=True,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n\n        source_ids = source[\"input_ids\"].squeeze()\n        source_mask = source[\"attention_mask\"].squeeze()\n\n        return {\n            \"source_ids\": source_ids.to(dtype=torch.long),\n            \"source_mask\": source_mask.to(dtype=torch.long),\n            \"labels\": torch.tensor(target_label, dtype=torch.long),\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def train(epoch, tokenizer, model, device, loader, optimizer):\n\n#     \"\"\"\n#     Function to be called for training with the parameters passed from main function\n#     \"\"\"\n\n#     model.train()\n#     for _, data in enumerate(loader, 0):\n#         y = data[\"labels\"].to(device, dtype=torch.long)\n#         print(\"y\",y)\n# #         y_ids = y[:, :-1].contiguous()\n# #         lm_labels = y[:, 1:].clone().detach()\n# #         lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n#         ids = data[\"source_ids\"].to(device, dtype=torch.long)\n#         mask = data[\"source_mask\"].to(device, dtype=torch.long)\n\n#         outputs = model(\n#             input_ids=ids,\n#             attention_mask=mask,\n#             labels=y,\n#         )\n#         loss = outputs[0]\n\n#         if _ % 10 == 0:\n#             training_logger.add_row(str(epoch), str(_), str(loss))\n#             console.print(training_logger)\n\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(epoch, model, device, loader, optimizer):\n    model.train()\n    for _, data in enumerate(loader, 0):\n        labels = data[\"labels\"].to(device, dtype=torch.long)\n        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        \n        del ids, mask, labels, outputs  # Free up memory\n        torch.cuda.empty_cache()\n\n        if _ % 10 == 0:\n            training_logger.add_row(str(epoch), str(_), str(loss.item()))\n            console.print(training_logger)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def validate(epoch, tokenizer, model, device, loader):\n\n#     \"\"\"\n#     Function to evaluate model for predictions\n#     \"\"\"\n\n#     model.eval()\n#     predictions = []\n#     actuals = []\n#     with torch.no_grad():\n#         for _, data in enumerate(loader, 0):\n#             y = data['labels'].to(device, dtype = torch.long)\n#             ids = data['source_ids'].to(device, dtype = torch.long)\n#             mask = data['source_mask'].to(device, dtype = torch.long)\n\n#             generated_ids = model.generate(\n#               input_ids = ids,\n#               attention_mask = mask, \n#               max_length=150, \n#               num_beams=2,\n#               repetition_penalty=2.5, \n#               length_penalty=1.0, \n#               early_stopping=True\n#               )\n\n#             preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n#             target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n#             if _%10==0:\n#                 print(f'Completed {_}')\n\n#             predictions.extend(preds)\n#             actuals.extend(target)\n#     return predictions, actuals","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(epoch, model, device, loader):\n    model.eval()\n    predictions = []\n    actuals = []\n    with torch.no_grad():\n        for _, data in enumerate(loader, 0):\n            labels = data[\"labels\"].to(device, dtype=torch.long)\n            ids = data[\"source_ids\"].to(device, dtype=torch.long)\n            mask = data[\"source_mask\"].to(device, dtype=torch.long)\n            \n            outputs = model(input_ids=ids, attention_mask=mask)\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1)\n            \n            predictions.extend(preds.cpu().numpy())\n            actuals.extend(labels.cpu().numpy())\n            if _ % 10 == 0:\n                print(f'Completed {_}')\n    \n    return predictions, actuals","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def T5Trainer(\n#     dataframe, source_text,tokenizer, target_text, model, model_params, output_dir=\"./outputs/\"\n# ):\n\n#     \"\"\"\n#     T5 trainer\n#     \"\"\"\n\n#     # Set random seeds and deterministic pytorch for reproducibility\n#     torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n#     np.random.seed(model_params[\"SEED\"])  # numpy random seed\n#     torch.backends.cudnn.deterministic = True\n\n#     # logging\n#     print(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n\n#     # tokenzier for encoding the text\n# #     tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n#     tokenizer = tokenizer\n\n#     # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n#     # Further this model is sent to device (GPU/TPU) for using the hardware.\n# #     model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n#     model = model\n#     model = model.to(device)\n\n#     # logging\n#     print(f\"[Data]: Reading data...\\n\")\n\n#     # Importing the raw dataset\n#     dataframe = dataframe[[source_text, target_text]]\n#     print(dataframe.head(2))\n\n#     # Creation of Dataset and Dataloader\n#     # Defining the train size. So 80% of the data will be used for training and the rest for validation.\n#     train_size = 0.8\n#     train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n#     val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n#     train_dataset = train_dataset.reset_index(drop=True)\n\n#     print(f\"FULL Dataset: {dataframe.shape}\")\n#     print(f\"TRAIN Dataset: {train_dataset.shape}\")\n#     print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n\n#     # Creating the Training and Validation dataset for further creation of Dataloader\n#     training_set = EventDataset(\n#         train_dataset,\n#         tokenizer,\n#         model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n#         source_text,\n#         target_text,\n#     )\n#     val_set = EventDataset(\n#         val_dataset,\n#         tokenizer,\n#         model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n#         source_text,\n#         target_text,\n#     )\n\n#     # Defining the parameters for creation of dataloaders\n#     train_params = {\n#         \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n#         \"shuffle\": True,\n#         \"num_workers\": 0,\n#     }\n\n#     val_params = {\n#         \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n#         \"shuffle\": False,\n#         \"num_workers\": 0,\n#     }\n\n#     # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n#     training_loader = DataLoader(training_set, **train_params)\n#     val_loader = DataLoader(val_set, **val_params)\n\n#     # Defining the optimizer that will be used to tune the weights of the network in the training session.\n#     optimizer = torch.optim.Adam(\n#         params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n#     )\n\n#     # Training loop\n#     print(f\"[Initiating Fine Tuning]...\\n\")\n\n#     for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n#         train(epoch, tokenizer, model, device, training_loader, optimizer)\n\n#     print(f\"[Saving Model]...\\n\")\n#     # Saving the model after training\n#     path = os.path.join(output_dir, \"model_files\")\n#     model.save_pretrained(path)\n#     tokenizer.save_pretrained(path)\n\n#     # evaluating test dataset\n#     print(f\"[Initiating Validation]...\\n\")\n#     for epoch in range(model_params[\"VAL_EPOCHS\"]):\n#         predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n#         final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n#         final_df.to_csv(os.path.join(output_dir, \"predictions.csv\"))\n\n# #     console.save_text(os.path.join(output_dir, \"logs.txt\"))\n\n#     print(f\"[Validation Completed.]\\n\")\n#     print(\n#         f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\"\n#     )\n#     print(\n#         f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\"\n#     )\n#     print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def T5Trainer(dataframe, source_text, tokenizer, target_text, model, model_params, output_dir=\"./outputs/\"):\n    torch.manual_seed(model_params[\"SEED\"])\n    np.random.seed(model_params[\"SEED\"])\n    torch.backends.cudnn.deterministic = True\n\n    print(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n\n    tokenizer = tokenizer\n    model = model.to(device)\n\n    print(f\"[Data]: Reading data...\\n\")\n\n    dataframe = dataframe[[source_text, target_text]]\n    print(dataframe.head(2))\n\n    train_size = 0.8\n    train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n    train_dataset = train_dataset.reset_index(drop=True)\n\n    print(f\"FULL Dataset: {dataframe.shape}\")\n    print(f\"TRAIN Dataset: {train_dataset.shape}\")\n    print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n\n    training_set = EventDataset(\n        train_dataset,\n        tokenizer,\n        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n        source_text,\n        target_text,\n    )\n\n    val_set = EventDataset(\n        val_dataset,\n        tokenizer,\n        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n        source_text,\n        target_text,\n    )\n\n    train_params = {\n        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n        \"shuffle\": True,\n        \"num_workers\": 0,\n    }\n\n    val_params = {\n        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n        \"shuffle\": False,\n        \"num_workers\": 0,\n    }\n\n    training_loader = DataLoader(training_set, **train_params)\n    val_loader = DataLoader(val_set, **val_params)\n\n    optimizer = torch.optim.Adam(\n        params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n    )\n\n    print(f\"[Initiating Fine Tuning]...\\n\")\n    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n        train(epoch, model, device, training_loader, optimizer)\n\n\n    print(f\"[Saving Model]...\\n\")\n    path = os.path.join(output_dir, \"model_files\")\n    model.save_pretrained(path)\n    tokenizer.save_pretrained(path)\n\n\n    print(f\"[Initiating Validation]...\\n\")\n    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n        predictions, actuals = validate(epoch, model, device, val_loader)\n        final_df = pd.DataFrame({\"Predictions\": predictions, \"Actuals\": actuals})\n        final_df.to_csv(os.path.join(output_dir, \"predictions.csv\"))\n\n    print(f\"[Validation Completed.]\\n\")\n    print(f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\")\n    print(f\"\"\"[Validation] Predictions on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\")\n    print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's define model parameters specific to T5\nmodel_params = {\n    \"MODEL\": \"t5-base\",  # model_type: t5-base/t5-large\n    \"TRAIN_BATCH_SIZE\": 8,  # training batch size\n    \"VALID_BATCH_SIZE\": 8,  # validation batch size\n    \"TRAIN_EPOCHS\": 3,  # number of training epochs\n    \"VAL_EPOCHS\": 1,  # number of validation epochs\n    \"LEARNING_RATE\": 1e-4,  # learning rate\n    \"MAX_SOURCE_TEXT_LENGTH\": 512,  # max length of source text\n    \"MAX_TARGET_TEXT_LENGTH\": 50,  # max length of target text\n    \"SEED\": 42,  # set seed for reproducibility\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's get a news summary dataset\n# dataframe has 2 columns: \n#   - text: long article content\n#   - headlines: one line summary of news\n# path = \"https://raw.githubusercontent.com/Shivanandroy/T5-Finetuning-PyTorch/main/data/news_summary.csv\"\n\n# df = pd.read_csv(path)\n\n# # T5 accepts prefix of the task to be performed:\n# # Since we are summarizing, let's add summarize to source text as a prefix\n# df[\"text\"] = \"summarize: \" + df[\"text\"]\n\n\nT5Trainer(\n    dataframe=unique_result,\n    source_text=\"text\",\n    tokenizer = tokenizer,\n    target_text=\"relation_to_id\",\n    model_params=model_params,\n    model = model,\n    output_dir=\"/kaggle/working/outputs\",\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = EventDataset(X_input_ids,X_attn_masks,labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader = DataLoader(dataset, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samples = next(iter(dataloader))\nsamples","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(epoch, model, data, optimizer):\n    \"\"\"\n    Function to be called for training with the parameters passed from main function\n    \"\"\"\n    model.train()\n    for _,data in enumerate(data, 0):\n        # y = data['labels'].to(device, dtype = torch.long)\n        # y_ids = y[:, :-1].contiguous()\n        # lm_labels = y[:, 1:].clone().detach()\n        # lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n        # ids = data['source_ids'].to(device, dtype = torch.long)\n        # mask = data['source_mask'].to(device, dtype = torch.long)\n        outputs = model(input_ids = data['input_ids'], attention_mask = data['attn_masks'], labels=data['labels'])\n        loss = outputs[0]\n\n    # if _%10==0:\n    #     training_logger.add_row(str(epoch), str(_), str(loss))\n    #     console.print(training_logger)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(epoch, tokenizer, model, device, loader):\n    \"\"\"\n    Function to evaluate model for predictions\n    \"\"\"\n    model.eval()\n    predictions = []\n    actuals = []\n    with torch.no_grad():\n        for _, data in enumerate(loader, 0):\n            y = data['target_ids'].to(device, dtype = torch.long)\n            ids = data['source_ids'].to(device, dtype = torch.long)\n            mask = data['source_mask'].to(device, dtype = torch.long)\n            generated_ids = model.generate(\n              input_ids = ids,\n              attention_mask = mask, \n              max_length=150,\n              num_beams=2,\n              repetition_penalty=2.5, \n              length_penalty=1.0, \n              early_stopping=True\n            )\n            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n            if _%10==0:\n                console.print(f'Completed {_}')\n            predictions.extend(preds)\n            actuals.extend(target)\n    return predictions, actuals","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}