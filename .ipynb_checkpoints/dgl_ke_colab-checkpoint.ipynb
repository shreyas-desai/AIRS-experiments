{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch_geometric"
      ],
      "metadata": {
        "id": "WYkfjZW4lWOP"
      },
      "id": "WYkfjZW4lWOP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5272e673-05f5-4100-bf34-0cfd88081721",
      "metadata": {
        "id": "5272e673-05f5-4100-bf34-0cfd88081721"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os.path as osp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy.stats import pearsonr\n",
        "from torch.utils.data import Dataset\n",
        "# from torch_geometric.utils import to_undirected, negative_sampling\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# from torch_geometric.datasets import FB15k_237\n",
        "# from torch_geometric.nn import ComplEx, DistMult, RotatE, TransE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb12c874-b712-4899-8fcf-dfa6d59d4a53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb12c874-b712-4899-8fcf-dfa6d59d4a53",
        "outputId": "796c66ca-bbc6-47b3-85e8-8e4e3644ca5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24d3c41f-cef2-458e-99d9-b0b620c32a6e",
      "metadata": {
        "id": "24d3c41f-cef2-458e-99d9-b0b620c32a6e"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"./quadruple_idx.txt\",sep = '\\t',names=['source', 'relation', 'destination','time'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "692c7753-48a6-4a7f-894f-6c4002055fd3",
      "metadata": {
        "id": "692c7753-48a6-4a7f-894f-6c4002055fd3"
      },
      "outputs": [],
      "source": [
        "triples = df[['source','relation','destination']].values\n",
        "triples, indices = np.unique(triples, return_index=True, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "522ada8b-9700-40e8-a844-55f6af646eb7",
      "metadata": {
        "id": "522ada8b-9700-40e8-a844-55f6af646eb7"
      },
      "outputs": [],
      "source": [
        "num_entities = len(np.unique(df[['source','relation','destination']].values))\n",
        "num_relations = len(np.unique(df[\"relation\"].values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04bcfa05-a915-4781-8486-f425a846c1d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04bcfa05-a915-4781-8486-f425a846c1d6",
        "outputId": "77d78097-ec2b-4950-bc47-6a642231b5ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6298, 234)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "num_entities, num_relations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "374bdf03-299c-4092-8710-fa7f6a15f903",
      "metadata": {
        "id": "374bdf03-299c-4092-8710-fa7f6a15f903"
      },
      "outputs": [],
      "source": [
        "class ICEWSDataset(Dataset):\n",
        "    def __init__(self, triples):\n",
        "        self.triples = triples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.triples[idx], dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f0e2d7c-540c-424d-8676-a35d17e970de",
      "metadata": {
        "id": "7f0e2d7c-540c-424d-8676-a35d17e970de"
      },
      "outputs": [],
      "source": [
        "def create_data_loaders(dataset, batch_size, validation_split=0.2):\n",
        "    dataset_size = len(dataset)\n",
        "    indices = list(range(dataset_size))\n",
        "    np.random.shuffle(indices)\n",
        "    split = int(np.floor(validation_split * dataset_size))\n",
        "    train_indices, val_indices = indices[split:], indices[:split]\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    valid_sampler = SubsetRandomSampler(val_indices)\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    valid_loader = DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler)\n",
        "    return train_loader, valid_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "900479f4-62b3-4d57-aad3-fb4ce23a9b10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "900479f4-62b3-4d57-aad3-fb4ce23a9b10",
        "outputId": "e05c811f-e7e5-438f-9462-73d820068e8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ],
      "source": [
        "icews_dataset = ICEWSDataset(triples)\n",
        "data_loader = DataLoader(icews_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b8b9cbc-04b8-4e3f-9398-6623e0935f99",
      "metadata": {
        "id": "7b8b9cbc-04b8-4e3f-9398-6623e0935f99"
      },
      "outputs": [],
      "source": [
        "class ComplExAttentionModel(nn.Module):\n",
        "    def __init__(self, num_entities, num_relations, embedding_dim, dropout_rate=0.3):\n",
        "        super(ComplExAttentionModel, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Embeddings for entities and relations (complex embeddings)\n",
        "        self.entity_embeddings_real = nn.Embedding(num_entities, embedding_dim)\n",
        "        self.entity_embeddings_imag = nn.Embedding(num_entities, embedding_dim)\n",
        "        self.relation_embeddings_real = nn.Embedding(num_relations, embedding_dim)\n",
        "        self.relation_embeddings_imag = nn.Embedding(num_relations, embedding_dim)\n",
        "        self.entity_bn = nn.BatchNorm1d(embedding_dim)\n",
        "        self.relation_bn = nn.BatchNorm1d(embedding_dim)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Attention Layer\n",
        "        self.attention_layer = nn.MultiheadAttention(embedding_dim, num_heads=8)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.entity_embeddings_real.weight)\n",
        "        nn.init.xavier_uniform_(self.entity_embeddings_imag.weight)\n",
        "        nn.init.xavier_uniform_(self.relation_embeddings_real.weight)\n",
        "        nn.init.xavier_uniform_(self.relation_embeddings_imag.weight)\n",
        "\n",
        "    def score(self, head, relation, tail):\n",
        "        # ComplEx scoring function\n",
        "        real_head = self.entity_embeddings_real(head)\n",
        "        imag_head = self.entity_embeddings_imag(head)\n",
        "        real_relation = self.relation_embeddings_real(relation)\n",
        "        imag_relation = self.relation_embeddings_imag(relation)\n",
        "        real_tail = self.entity_embeddings_real(tail)\n",
        "        imag_tail = self.entity_embeddings_imag(tail)\n",
        "\n",
        "        # Apply batch normalization on embeddings\n",
        "        real_head = self.entity_bn(real_head)\n",
        "        imag_head = self.entity_bn(imag_head)\n",
        "        real_relation = self.relation_bn(real_relation)\n",
        "        imag_relation = self.relation_bn(imag_relation)\n",
        "        real_tail = self.entity_bn(real_tail)\n",
        "        imag_tail = self.entity_bn(imag_tail)\n",
        "\n",
        "        # ComplEx score computation\n",
        "        score_real = torch.sum(real_head * real_relation * real_tail + imag_head * imag_relation * imag_tail, dim=-1)\n",
        "        score_imag = torch.sum(real_head * imag_relation * imag_tail - imag_head * real_relation * real_tail, dim=-1)\n",
        "\n",
        "        return score_real + score_imag\n",
        "\n",
        "    def forward(self, head, relation):\n",
        "        # Get embeddings for head and relation\n",
        "        real_head = self.entity_embeddings_real(head)\n",
        "        imag_head = self.entity_embeddings_imag(head)\n",
        "        real_relation = self.relation_embeddings_real(relation)\n",
        "        imag_relation = self.relation_embeddings_imag(relation)\n",
        "\n",
        "        # Apply batch normalization\n",
        "        real_head = self.entity_bn(real_head)\n",
        "        imag_head = self.entity_bn(imag_head)\n",
        "        real_relation = self.relation_bn(real_relation)\n",
        "        imag_relation = self.relation_bn(imag_relation)\n",
        "\n",
        "        # Apply dropout\n",
        "        real_head = self.dropout(real_head)\n",
        "        imag_head = self.dropout(imag_head)\n",
        "        real_relation = self.dropout(real_relation)\n",
        "        imag_relation = self.dropout(imag_relation)\n",
        "\n",
        "        # Compute attention over all entity embeddings\n",
        "        entity_real = self.entity_embeddings_real.weight.unsqueeze(1)\n",
        "        entity_imag = self.entity_embeddings_imag.weight.unsqueeze(1)\n",
        "\n",
        "        query_real = real_head + real_relation\n",
        "        query_imag = imag_head + imag_relation\n",
        "\n",
        "        query = query_real + query_imag  # Combine real and imaginary for attention input\n",
        "        key = entity_real + entity_imag   # Keys are all entities in the graph\n",
        "\n",
        "        # Apply attention mechanism\n",
        "        attention_output, attention_weights = self.attention_layer(query.unsqueeze(1), key, key)\n",
        "\n",
        "        # Use attention output to predict most likely tail (object entity)\n",
        "        scores = torch.matmul(attention_output.squeeze(1), (entity_real + entity_imag).squeeze(1).T)\n",
        "        return scores, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e24613b-2165-496a-a100-fd200599032c",
      "metadata": {
        "id": "0e24613b-2165-496a-a100-fd200599032c"
      },
      "outputs": [],
      "source": [
        "def rank_predictions(scores, true_tail):\n",
        "    sorted_scores, sorted_indices = torch.sort(scores, descending=True)\n",
        "    true_rank = (sorted_indices == true_tail).nonzero(as_tuple=True)[0].item() + 1\n",
        "    return true_rank, sorted_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80c6c5a0-a444-41e5-ba16-255fede505d1",
      "metadata": {
        "id": "80c6c5a0-a444-41e5-ba16-255fede505d1"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, data_loader, k=10):\n",
        "    model.eval()\n",
        "    total_mrr = 0\n",
        "    total_hits_at_k = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            head = batch[:, 0].to(device)\n",
        "            relation = batch[:, 1].to(device)\n",
        "            tail = batch[:, 2].to(device)\n",
        "\n",
        "            scores, _ = model(head, relation)\n",
        "\n",
        "            for i in range(len(tail)):\n",
        "                true_tail = tail[i]\n",
        "                true_rank, sorted_indices = rank_predictions(scores[i], true_tail)\n",
        "                total_mrr += 1.0 / true_rank\n",
        "                if true_tail in sorted_indices[:k]:\n",
        "                    total_hits_at_k += 1\n",
        "                num_samples += 1\n",
        "\n",
        "    mrr = total_mrr / num_samples\n",
        "    hits_at_k = total_hits_at_k / num_samples\n",
        "    return mrr, hits_at_k\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_negative_samples(batch, num_entities):\n",
        "    \"\"\"\n",
        "    Generate negative samples by corrupting either the head or the tail of each triple.\n",
        "\n",
        "    :param batch: A batch of triples (h, r, t)\n",
        "    :param num_entities: Total number of entities in the knowledge graph\n",
        "    :return: Negative samples (same size as the batch)\n",
        "    \"\"\"\n",
        "    negative_batch = batch.clone()\n",
        "\n",
        "    # Randomly corrupt head or tail for each triple\n",
        "    for i in range(batch.size(0)):\n",
        "        corrupt_head = random.choice([True, False])\n",
        "        if corrupt_head:\n",
        "            # Replace the head entity with a random entity\n",
        "            negative_batch[i, 0] = random.randint(0, num_entities - 1)\n",
        "        else:\n",
        "            # Replace the tail entity with a random entity\n",
        "            negative_batch[i, 2] = random.randint(0, num_entities - 1)\n",
        "\n",
        "    return negative_batch\n"
      ],
      "metadata": {
        "id": "tXh7OtHrYykQ"
      },
      "id": "tXh7OtHrYykQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "679ccaa0-8569-468a-aa0f-2b7a08010ae9",
      "metadata": {
        "id": "679ccaa0-8569-468a-aa0f-2b7a08010ae9"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, optimizer, criterion, num_epochs=30, k=10, weight_decay=1e-5, lr_scheduler_step=10, lr_decay_factor=0.5):\n",
        "    \"\"\"\n",
        "    Train the model with learning rate scheduling and weight decay.\n",
        "    :param model: The knowledge graph model (e.g., ComplExAttentionModel).\n",
        "    :param data_loader: DataLoader containing training data.\n",
        "    :param optimizer: Optimizer for training (e.g., Adam).\n",
        "    :param criterion: Loss function (e.g., CrossEntropyLoss).\n",
        "    :param num_epochs: Number of training epochs.\n",
        "    :param k: Top-K accuracy for Hits@k.\n",
        "    :param weight_decay: L2 regularization term.\n",
        "    :param lr_scheduler_step: Number of epochs after which to decay the learning rate.\n",
        "    :param lr_decay_factor: Factor by which to reduce the learning rate.\n",
        "    \"\"\"\n",
        "    # Split the data into train and validation sets\n",
        "    train_loader, valid_loader = create_data_loaders(data_loader.dataset, batch_size=data_loader.batch_size)\n",
        "\n",
        "    # Initialize learning rate scheduler\n",
        "    scheduler = StepLR(optimizer, step_size=lr_scheduler_step, gamma=lr_decay_factor)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        # Training loop\n",
        "        for batch in train_loader:\n",
        "            head, relation, tail = batch[:, 0].to(device), batch[:, 1].to(device), batch[:, 2].to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass: Get scores from the model\n",
        "            scores, attention_weights = model(head, relation)\n",
        "\n",
        "            # Compute the loss between predicted scores and true tail entities\n",
        "            loss = criterion(scores, tail)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Step the scheduler to adjust learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Evaluation after each epoch\n",
        "        mrr, hits_at_k = evaluate_model(model, valid_loader, k=k)\n",
        "\n",
        "        # Print loss and evaluation metrics for this epoch\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}, MRR: {mrr:.4f}, Hits@{k}: {hits_at_k:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1836872-f580-47d9-bf6c-e3b614a76c50",
      "metadata": {
        "id": "a1836872-f580-47d9-bf6c-e3b614a76c50"
      },
      "outputs": [],
      "source": [
        "# def train(model, data_loader, optimizer, criterion, num_epochs=10, k=10):\n",
        "#     train_loader, valid_loader = create_data_loaders(data_loader.dataset, batch_size=data_loader.batch_size)\n",
        "#     for epoch in range(num_epochs):\n",
        "#         total_loss = 0\n",
        "#         model.train()\n",
        "#         for batch in train_loader:\n",
        "#             head, relation, tail = batch[:, 0].to(device), batch[:, 1].to(device), batch[:, 2].to(device)\n",
        "#             # head, relation, tail = batch[:, 0], batch[:, 1], batch[:, 2]\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             scores, attention_weights = model(head, relation)\n",
        "#             loss = criterion(scores, tail)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             total_loss += loss.item()\n",
        "#         mrr, hits_at_k = evaluate_model(model, head, relation, tail, valid_loader, k=k)\n",
        "\n",
        "#         print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}, MRR: {mrr:.4f}, Hits@{k}: {hits_at_k:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc5fc709-4b33-426d-a25b-f1e18c93e603",
      "metadata": {
        "id": "dc5fc709-4b33-426d-a25b-f1e18c93e603"
      },
      "outputs": [],
      "source": [
        "# Define criterion and optimizer\n",
        "model = ComplExAttentionModel(num_entities=num_entities, num_relations=num_relations, embedding_dim=256).to(device)\n",
        "# model = ComplExAttentionModel(num_entities=num_entities, num_relations=num_relations, embedding_dim=64)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a0ba238-13a1-466c-b8e8-d02f65c3ade2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a0ba238-13a1-466c-b8e8-d02f65c3ade2",
        "outputId": "afdabfc2-ea95-49f2-97f7-97d1728bbcd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 17070.7514, MRR: 0.1716, Hits@10: 0.3078\n",
            "Epoch 2/100, Loss: 16230.6237, MRR: 0.1741, Hits@10: 0.3124\n",
            "Epoch 3/100, Loss: 15993.8062, MRR: 0.1762, Hits@10: 0.3196\n",
            "Epoch 4/100, Loss: 15806.1288, MRR: 0.1780, Hits@10: 0.3177\n",
            "Epoch 5/100, Loss: 15651.6520, MRR: 0.1811, Hits@10: 0.3262\n",
            "Epoch 6/100, Loss: 15521.9371, MRR: 0.1798, Hits@10: 0.3259\n",
            "Epoch 7/100, Loss: 15415.4363, MRR: 0.1819, Hits@10: 0.3292\n"
          ]
        }
      ],
      "source": [
        "train(model, data_loader, optimizer, criterion, num_epochs=100, weight_decay=1e-4, lr_scheduler_step=10, lr_decay_factor=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa51270f-1e23-494c-998a-7f2592f1a2ed",
      "metadata": {
        "id": "aa51270f-1e23-494c-998a-7f2592f1a2ed"
      },
      "outputs": [],
      "source": [
        "entity_map = {}\n",
        "relation_map = {}\n",
        "with open(\"./entity2id.txt\",'r',encoding='utf-8') as file:\n",
        "    for line in file.readlines():\n",
        "        entity_map[int(line.split(\"\\t\")[1].strip())] = line.split(\"\\t\")[0]\n",
        "\n",
        "with open(\"./relation2id.txt\",'r',encoding='utf-8') as file:\n",
        "    for line in file.readlines():\n",
        "        relation_map[int(line.split(\"\\t\")[1].strip())] = line.split(\"\\t\")[0]\n",
        "\n",
        "def get_real_facts(triple):\n",
        "    return entity_map[triple[0]],relation_map[triple[1]],entity_map[triple[2]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "280161e1-5304-4aeb-b876-3c3b95d85fd4",
      "metadata": {
        "id": "280161e1-5304-4aeb-b876-3c3b95d85fd4"
      },
      "outputs": [],
      "source": [
        "def get_correlated_event_triples(model, head, relation, tail, triples, top_k=5):\n",
        "    \"\"\"\n",
        "    Get the correlated event triples for a given fact (head, relation, tail) using attention weights.\n",
        "    :param model: Trained ComplExAttentionModel\n",
        "    :param head: Tensor containing the head entity\n",
        "    :param relation: Tensor containing the relation\n",
        "    :param tail: Tensor containing the true tail entity (optional for prediction)\n",
        "    :param triples: Array of all known triples (head, relation, tail)\n",
        "    :param top_k: Number of top correlated events to return\n",
        "    :return: top_k_event_triples (correlated event triples), correlated_weights (attention weights)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Get scores and attention weights for the query (head, relation)\n",
        "        scores, attention_weights = model(head, relation)\n",
        "\n",
        "        # The attention weights are for entities, but we want to map them back to triples\n",
        "        entity_real = model.entity_embeddings_real.weight.unsqueeze(1)\n",
        "        entity_imag = model.entity_embeddings_imag.weight.unsqueeze(1)\n",
        "\n",
        "        # Combine real and imaginary parts of the entities to form full embeddings\n",
        "        entity_full = entity_real + entity_imag\n",
        "\n",
        "        # Reshape attention weights to align with the entity space\n",
        "        attention_weights = attention_weights.squeeze()  # Remove any singleton dimensions\n",
        "\n",
        "        # Track which triples got the most attention, we will use `torch.topk` to find top-K attention weights\n",
        "        correlated_triples = []\n",
        "        correlated_weights = []\n",
        "\n",
        "        # Loop through the known triples and gather the attention weights associated with the head, relation, and tail\n",
        "        for i, (h, r, t) in enumerate(triples):\n",
        "            attention_head = attention_weights[h]\n",
        "            attention_tail = attention_weights[t]\n",
        "            combined_attention = attention_head + attention_tail  # Combine attention for head and tail\n",
        "\n",
        "            correlated_triples.append((h, r, t))\n",
        "            correlated_weights.append(combined_attention)\n",
        "\n",
        "        # Convert to tensor for easy processing\n",
        "        correlated_weights = torch.stack(correlated_weights)\n",
        "\n",
        "        # Get the top-K triples with the highest combined attention weights\n",
        "        top_k_weights, top_k_indices = torch.topk(correlated_weights, k=top_k)\n",
        "        top_k_triples = [correlated_triples[idx] for idx in top_k_indices]\n",
        "\n",
        "        actual_tail_attention_weight = attention_weights[tail].item()\n",
        "        print(f\"Attention weight for true tail entity ({tail.item()}): {actual_tail_attention_weight}\")\n",
        "\n",
        "        for i, (triple, weight) in enumerate(zip(top_k_triples, top_k_weights)):\n",
        "            print(f\"Correlated event triple {i+1}: {get_real_facts(triple)}, Attention weight: {weight.item()}\")\n",
        "\n",
        "        return top_k_triples, top_k_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccc558b1-8f2a-4aa2-b594-3faec238634d",
      "metadata": {
        "id": "ccc558b1-8f2a-4aa2-b594-3faec238634d"
      },
      "outputs": [],
      "source": [
        "# Example query with full fact\n",
        "head = torch.tensor([132]).to(device)  # Example head entity\n",
        "relation = torch.tensor([9]).to(device)  # Example relation\n",
        "tail = torch.tensor([1]).to(device)  # Example true tail entity\n",
        "\n",
        "# head = torch.tensor([31])  # Example head entity\n",
        "# relation = torch.tensor([58]) # Example relation\n",
        "# tail = torch.tensor([1])  # Example true tail entity\n",
        "\n",
        "print(f\"Query triple:{get_real_facts((head.item(),relation.item(),tail.item()))}\")\n",
        "# Call the function with the complete fact (head, relation, tail)\n",
        "correlated_event_triples, correlated_weights = get_correlated_event_triples(model, head, relation, tail, triples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3347e818-2ebe-49e4-93d3-3cb46adfe33d",
      "metadata": {
        "id": "3347e818-2ebe-49e4-93d3-3cb46adfe33d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}