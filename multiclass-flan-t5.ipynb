{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95b57bc0-6079-4c5e-8d89-5ea632d253a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 30 15:20:08 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 556.12                 Driver Version: 556.12         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   43C    P8              2W /   50W |     244MiB /   4096MiB |     18%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      3472    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A      4616    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     17424    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     18456    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be7910f-0378-4b44-88d9-0f8946dd92eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c7b456d-6e2e-4920-b5f7-826ede68e5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ed6a05b-b254-409e-94f8-8327f59a81e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f925cd51-b266-4320-9c02-ece514ec710b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>relation</th>\n",
       "      <th>target</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Armed Gang (Afghanistan)</td>\n",
       "      <td>Carry out suicide bombing</td>\n",
       "      <td>United States</td>\n",
       "      <td>2010-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Central Intelligence Agency</td>\n",
       "      <td>Make statement</td>\n",
       "      <td>Taliban</td>\n",
       "      <td>2010-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Taliban</td>\n",
       "      <td>Make statement</td>\n",
       "      <td>Attacker (Afghanistan)</td>\n",
       "      <td>2010-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Citizen (Afghanistan)</td>\n",
       "      <td>Demonstrate or rally</td>\n",
       "      <td>Unspecified Actor</td>\n",
       "      <td>2010-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Armed Gang (Afghanistan)</td>\n",
       "      <td>Carry out suicide bombing</td>\n",
       "      <td>Central Intelligence Agency</td>\n",
       "      <td>2010-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        source                   relation  \\\n",
       "0     Armed Gang (Afghanistan)  Carry out suicide bombing   \n",
       "1  Central Intelligence Agency             Make statement   \n",
       "2                      Taliban             Make statement   \n",
       "3        Citizen (Afghanistan)       Demonstrate or rally   \n",
       "4     Armed Gang (Afghanistan)  Carry out suicide bombing   \n",
       "\n",
       "                        target        date  \n",
       "0                United States  2010-01-01  \n",
       "1                      Taliban  2010-01-01  \n",
       "2       Attacker (Afghanistan)  2010-01-01  \n",
       "3            Unspecified Actor  2010-01-01  \n",
       "4  Central Intelligence Agency  2010-01-01  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/raw_data/rawdat/AFG/quadruple.txt\", sep='\\t', names = [\"source\",\"relation\",\"target\",\"date\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff8b4ef6-c029-417d-a22f-3d0d2fbe7392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to a foreign news agency, a suicide ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The CIA said a Taliban bomber on Wednesday man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Taliban claimed responsibility for the att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The case in Kunar has already prompted Afghans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The CIA base attacked by a suicide bomber in A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  According to a foreign news agency, a suicide ...\n",
       "1  The CIA said a Taliban bomber on Wednesday man...\n",
       "2  The Taliban claimed responsibility for the att...\n",
       "3  The case in Kunar has already prompted Afghans...\n",
       "4  The CIA base attacked by a suicide bomber in A..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =pd.read_csv(\"../data/raw_data/rawdat/AFG/text.txt\", sep='\\t', names = [\"text\"])\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59836728-f2b6-47d1-96b3-cbf74673d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['source'] = data['source'].str.strip()\n",
    "data['relation'] = data['relation'].str.strip()\n",
    "data['target'] = data['target'].str.strip()\n",
    "text['text'] = text['text'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed769db3-ad08-4640-b248-efcb560e7a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>relation</th>\n",
       "      <th>target</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Armed Gang (Afghanistan)</td>\n",
       "      <td>Carry out suicide bombing</td>\n",
       "      <td>United States</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>According to a foreign news agency, a suicide ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Central Intelligence Agency</td>\n",
       "      <td>Make statement</td>\n",
       "      <td>Taliban</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>The CIA said a Taliban bomber on Wednesday man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Taliban</td>\n",
       "      <td>Make statement</td>\n",
       "      <td>Attacker (Afghanistan)</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>The Taliban claimed responsibility for the att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Citizen (Afghanistan)</td>\n",
       "      <td>Demonstrate or rally</td>\n",
       "      <td>Unspecified Actor</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>The case in Kunar has already prompted Afghans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Armed Gang (Afghanistan)</td>\n",
       "      <td>Carry out suicide bombing</td>\n",
       "      <td>Central Intelligence Agency</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>The CIA base attacked by a suicide bomber in A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        source                   relation  \\\n",
       "0     Armed Gang (Afghanistan)  Carry out suicide bombing   \n",
       "1  Central Intelligence Agency             Make statement   \n",
       "2                      Taliban             Make statement   \n",
       "3        Citizen (Afghanistan)       Demonstrate or rally   \n",
       "4     Armed Gang (Afghanistan)  Carry out suicide bombing   \n",
       "\n",
       "                        target        date  \\\n",
       "0                United States  2010-01-01   \n",
       "1                      Taliban  2010-01-01   \n",
       "2       Attacker (Afghanistan)  2010-01-01   \n",
       "3            Unspecified Actor  2010-01-01   \n",
       "4  Central Intelligence Agency  2010-01-01   \n",
       "\n",
       "                                                text  \n",
       "0  According to a foreign news agency, a suicide ...  \n",
       "1  The CIA said a Taliban bomber on Wednesday man...  \n",
       "2  The Taliban claimed responsibility for the att...  \n",
       "3  The case in Kunar has already prompted Afghans...  \n",
       "4  The CIA base attacked by a suicide bomber in A...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.concat([data,text],axis=1)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "993710d5-d5a2-4327-9231-de8d2395ab1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relation\n",
       "Make statement                                   53958\n",
       "Use conventional military force                  33243\n",
       "Consult                                          26085\n",
       "Use unconventional violence                      15225\n",
       "Make an appeal or request                        14141\n",
       "                                                 ...  \n",
       "Forgive                                              1\n",
       "Ease political dissent                               1\n",
       "Reject request or demand for political reform        1\n",
       "Reject mediation                                     1\n",
       "Investigate war crimes                               1\n",
       "Name: count, Length: 218, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "601cdce5-0d0d-4afb-ba64-24f70f162419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source      279859\n",
       "relation    279859\n",
       "target      279859\n",
       "date        279859\n",
       "text        279859\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "191d9830-a063-4807-8dcb-073c91ba6a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.to_csv(\"../data/AFG_data_quintuples/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2ccd75e-8363-4f3d-957f-afb4558023d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desai\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\desai\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f6241e6-03a2-483b-a53e-a5420b59203b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Carry out suicide bombing', 'Make statement',\n",
       "       'Demonstrate or rally', \"Meet at a 'third' location\",\n",
       "       'Employ aerial weapons', 'Acknowledge or claim responsibility',\n",
       "       'Engage in negotiation', 'Make an appeal or request',\n",
       "       'Use conventional military force', 'Consult',\n",
       "       'Threaten with military force',\n",
       "       'Mobilize or increase armed forces',\n",
       "       'Abduct, hijack, or take hostage', 'Use unconventional violence',\n",
       "       'Make a visit', 'Host a visit',\n",
       "       'Appeal for diplomatic cooperation (such as policy support)',\n",
       "       'Provide aid',\n",
       "       'Conduct suicide, car, or other non-military bombing',\n",
       "       'Investigate', 'Accuse', 'Criticize or denounce', 'Reject',\n",
       "       'Arrest, detain, or charge with legal action', 'Praise or endorse',\n",
       "       'Demand', 'fight with artillery and tanks', 'Deny responsibility',\n",
       "       'Use as human shield', 'Accuse of human rights abuses',\n",
       "       'fight with small arms and light weapons', 'Cooperate militarily',\n",
       "       'Express intent to engage in diplomatic cooperation (such as policy support)',\n",
       "       'Investigate crime, corruption', 'Make pessimistic comment',\n",
       "       'Express intent to provide economic aid', 'Provide economic aid',\n",
       "       'Engage in diplomatic cooperation',\n",
       "       'Express intent to meet or negotiate',\n",
       "       'Express intent to institute political reform',\n",
       "       'Make empathetic comment',\n",
       "       'Reject proposal to meet, discuss, or negotiate',\n",
       "       'Demand de-escalation of military engagement', 'Destroy property',\n",
       "       'Threaten', 'Provide military aid', 'Discuss by telephone',\n",
       "       'Express intent to provide military aid', 'Confiscate property',\n",
       "       'Retreat or surrender militarily',\n",
       "       'Increase military alert status', 'Physically assault',\n",
       "       'Apologize', 'Protest violently, riot',\n",
       "       'Share intelligence or information',\n",
       "       'Provide military protection or peacekeeping',\n",
       "       'Express intent to yield', 'Sign formal agreement',\n",
       "       'Express intent to provide military protection or peacekeeping',\n",
       "       'Receive deployment of peacekeepers', 'Grant asylum',\n",
       "       'Engage in material cooperation', 'Expel or deport individuals',\n",
       "       'Engage in symbolic act', 'Reduce relations',\n",
       "       'Express intent to cooperate', 'Make optimistic comment',\n",
       "       'Return, release property', 'Increase police alert status',\n",
       "       'Provide humanitarian aid',\n",
       "       'Appeal for de-escalation of military engagement',\n",
       "       'Return, release person(s)', 'Rally support on behalf of',\n",
       "       'Appeal to yield', 'Demobilize armed forces', 'Occupy territory',\n",
       "       'Express intent to settle dispute', 'Appeal for intelligence',\n",
       "       'Conduct strike or boycott', 'Obstruct passage, block',\n",
       "       'Reject plan, agreement to settle dispute',\n",
       "       'Express intent to cooperate on judicial matters',\n",
       "       'Consider policy option', 'Impose administrative sanctions',\n",
       "       'Defy norms, law', 'Mediate', 'Demand intelligence cooperation',\n",
       "       'Kill by physical assault', 'Coerce',\n",
       "       'Express intent to de-escalate military engagement',\n",
       "       'Express intent to cooperate on intelligence',\n",
       "       'Reduce or break diplomatic relations',\n",
       "       'Threaten to reduce or break relations',\n",
       "       'Appeal to others to meet or negotiate',\n",
       "       'Grant diplomatic recognition',\n",
       "       'Demonstrate military or police power',\n",
       "       'Express intent to cooperate economically', 'Yield',\n",
       "       'Appeal to engage in or accept mediation',\n",
       "       'Impose restrictions on political freedoms',\n",
       "       'Refuse to de-escalate military engagement',\n",
       "       'Appeal for material cooperation', 'Express intent to mediate',\n",
       "       'Mobilize or increase police power', 'Rally opposition against',\n",
       "       'Demand judicial cooperation', 'Defend verbally',\n",
       "       'Receive inspectors', 'Appeal for economic aid', 'Decline comment',\n",
       "       'Seize or damage property',\n",
       "       'Express intent to provide material aid',\n",
       "       'Impose blockade, restrict movement', 'Appeal for aid',\n",
       "       'Demand change in leadership',\n",
       "       'Demand diplomatic cooperation (such as policy support)',\n",
       "       'Bring lawsuit against', 'Carry out roadside bombing',\n",
       "       'Cooperate economically',\n",
       "       'Accede to demands for change in leadership',\n",
       "       'Accede to demands for rights',\n",
       "       'Threaten with political dissent, protest', 'Assassinate',\n",
       "       'Expel or withdraw peacekeepers',\n",
       "       'Appeal for change in leadership', 'Carry out car bombing',\n",
       "       'Appeal for easing of administrative sanctions', 'Refuse to yield',\n",
       "       'Impose embargo, boycott, or sanctions',\n",
       "       'Express intent to provide humanitarian aid', 'Sexually assault',\n",
       "       'Appeal for judicial cooperation', 'Demand material cooperation',\n",
       "       'Give ultimatum', 'Refuse to release persons or property',\n",
       "       'Appeal for humanitarian aid', 'Accuse of war crimes',\n",
       "       'Accuse of crime, corruption', 'Halt negotiations',\n",
       "       'Demand meeting, negotiation',\n",
       "       'Express intent to release persons or property',\n",
       "       'Appeal for economic cooperation', 'Demand political reform',\n",
       "       'Declare truce, ceasefire',\n",
       "       'Reduce or stop humanitarian assistance',\n",
       "       'Reject request for policy change', 'Attempt to assassinate',\n",
       "       'Expel or withdraw', 'Express accord', 'Appeal for policy change',\n",
       "       'Impose state of emergency or martial law',\n",
       "       'Reject request for change in leadership',\n",
       "       'Express intent to change policy',\n",
       "       'Express intent to change leadership',\n",
       "       'Appeal to others to settle dispute',\n",
       "       'Threaten with sanctions, boycott, embargo',\n",
       "       'Accuse of espionage, treason',\n",
       "       'Demand release of persons or property', 'Demand rights',\n",
       "       'Appeal for military aid', 'Use tactics of violent repression',\n",
       "       'Threaten to reduce or stop aid', 'Torture',\n",
       "       'Ease administrative sanctions',\n",
       "       'Express intent to engage in material cooperation',\n",
       "       'Reduce or stop material aid',\n",
       "       'Express intent to ease economic sanctions, boycott, or embargo',\n",
       "       'Reduce or stop economic assistance', 'Conduct hunger strike',\n",
       "       'Demonstrate for policy change',\n",
       "       'Engage in violent protest for policy change',\n",
       "       'Engage in mass killings',\n",
       "       'Appeal for release of persons or property',\n",
       "       'Investigate human rights abuses', 'Demand economic aid',\n",
       "       'Express intent to accept mediation', 'Threaten to halt mediation',\n",
       "       'Halt mediation', 'Demand mediation',\n",
       "       'Engage in judicial cooperation',\n",
       "       'Express intent to cooperate militarily',\n",
       "       'Threaten with administrative sanctions', 'Complain officially',\n",
       "       'Ease military blockade',\n",
       "       'Appeal for military protection or peacekeeping',\n",
       "       'Accede to demands for change in policy',\n",
       "       'Threaten with restrictions on political freedoms',\n",
       "       'Demonstrate for leadership change',\n",
       "       'Appeal for easing of political dissent',\n",
       "       'Accede to requests or demands for political reform',\n",
       "       'Reject economic cooperation',\n",
       "       'Reject request or demand for political reform',\n",
       "       'Ease economic sanctions, boycott, embargo',\n",
       "       'Investigate military action', 'Demand policy change',\n",
       "       'Threaten to impose state of emergency or martial law',\n",
       "       'Appeal for change in institutions, regime',\n",
       "       'Ban political parties or politicians', 'Demand material aid',\n",
       "       'Impose curfew', 'Ease political dissent',\n",
       "       'Threaten with repression',\n",
       "       'Express intent to ease administrative sanctions',\n",
       "       'Demand economic cooperation', 'Appeal for political reform',\n",
       "       'Demand settling of dispute', 'Reduce or stop military assistance',\n",
       "       'Demand military aid', 'Appeal for rights',\n",
       "       'Refuse to ease economic sanctions, boycott, or embargo',\n",
       "       'Reject request for military aid',\n",
       "       'Reject request or demand for material aid', 'Forgive',\n",
       "       'Demand change in institutions, regime',\n",
       "       'Express intent to change institutions, regime',\n",
       "       'Reject mediation', 'Demand that target yields',\n",
       "       'Investigate war crimes'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.relation.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca0d618f-2dc3-417d-a893-0d5b9b028eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carry out suicide bombing   [11274, 63, 91, 12259, 6417, 53, 1]\n",
      "Make statement   [1796, 2493, 1]\n",
      "Demonstrate or rally   [15782, 29, 7, 17, 2206, 42, 13980, 1]\n",
      "Meet at a 'third' location   [12325, 44, 3, 9, 3, 31, 14965, 31, 1128, 1]\n",
      "Employ aerial weapons   [19631, 22142, 7749, 1]\n",
      "Acknowledge or claim responsibility   [4292, 20542, 13553, 42, 1988, 3263, 1]\n",
      "Engage in negotiation   [27246, 15, 16, 21862, 1]\n",
      "Make an appeal or request   [1796, 46, 3958, 42, 1690, 1]\n",
      "Use conventional military force   [2048, 7450, 2716, 2054, 1]\n",
      "Consult   [11540, 17, 1]\n"
     ]
    }
   ],
   "source": [
    "relations = result.relation.unique()\n",
    "for r in relations[:10]:\n",
    "    print(r,\" \",tokenizer.encode(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f09727ad-1e80-4009-a49e-58c7a501e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data_dir, type_path,  max_len=512):\n",
    "        self.path = os.path.join(data_dir, type_path + '.csv')\n",
    "        self.data_column = \"text\"\n",
    "        self.class_column = \"relation\"\n",
    "        self.data = pd.read_csv(self.path,names=[\"source\",\"relation\",\"target\",\"date\",\"text\"])\n",
    "        # self.data.drop_duplicates(inplace=True)\n",
    "        relation_ids = pd.read_csv('../data/raw_data/rawdat/AFG/relation2id.txt', sep='\\t', names=['relation','id'])\n",
    "        relation_id_maps = pd.Series(relation_ids.id.values, index=relation_ids.relation).to_dict()\n",
    "        self.data['relation'] = self.data['relation'].map(relation_id_maps).astype('str')\n",
    "        self.data.head()\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "    \n",
    "        self._build()\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "        \n",
    "        src_mask    = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "        \n",
    "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "  \n",
    "    def _build(self):\n",
    "        for idx in range(len(self.data)):\n",
    "            input_, target = self.data.loc[idx, self.data_column], self.data.loc[idx, self.class_column]      \n",
    "            \n",
    "            input_ = input_ + ' '\n",
    "            target = target + \" \"\n",
    "            \n",
    "            # tokenize inputs\n",
    "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "              [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            # tokenize targets\n",
    "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "              [target], pad_to_max_length=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            self.inputs.append(tokenized_inputs)\n",
    "            self.targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5619a22-0b96-49de-bed3-2cffcb7154cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(T5FineTuner, self).__init__()\n",
    "        # print(hparams)\n",
    "        self.save_hyperparameters(hparams)\n",
    "        \n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
    "  \n",
    "    def is_logger(self):\n",
    "        return self.trainer.proc_rank <= 0\n",
    "  \n",
    "    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            lm_labels=lm_labels,\n",
    "        )\n",
    "\n",
    "    def _step(self, batch):\n",
    "        lm_labels = batch[\"target_ids\"]\n",
    "        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "        outputs = self(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            lm_labels=lm_labels,\n",
    "            decoder_attention_mask=batch['target_mask']\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        \n",
    "        tensorboard_logs = {\"train_loss\": loss}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "  \n",
    "    def on_train_epoch_end(self, outputs):\n",
    "        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "        return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        return {\"val_loss\": loss}\n",
    "  \n",
    "    def on_validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "        \n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "  \n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
    "        if self.trainer.use_tpu:\n",
    "            xm.optimizer_step(optimizer)\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        self.lr_scheduler.step()\n",
    "\n",
    "    def get_tqdm_dict(self):\n",
    "        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "        \n",
    "        return tqdm_dict\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n",
    "        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=4,persistent_workers=True)\n",
    "        t_total = (\n",
    "            (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
    "            // self.hparams.gradient_accumulation_steps\n",
    "            * float(self.hparams.num_train_epochs)\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"val\", args=self.hparams)\n",
    "        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4,persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46253e0f-6ca4-4956-9a3e-f94e98ebc918",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoggingCallback(pl.Callback):\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        logger.info(\"***** Validation results *****\")\n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "        # Log results\n",
    "        for key in sorted(metrics):\n",
    "            if key not in [\"log\", \"progress_bar\"]:\n",
    "                logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "\n",
    "    def on_test_end(self, trainer, pl_module):\n",
    "        logger.info(\"***** Test results *****\")\n",
    "\n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "\n",
    "        # Log and save results to file\n",
    "        output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
    "        with open(output_test_results_file, \"w\") as writer:\n",
    "            for key in sorted(metrics):\n",
    "                if key not in [\"log\", \"progress_bar\"]:\n",
    "                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "                    writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b546fab-cce5-44dd-b5cc-133f226ca4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    data_dir=\"../data/AFG_data_quintuples/\", # path for data files\n",
    "    output_dir=\"../data/outputs/\", # path to save the checkpoints\n",
    "    model_name_or_path='t5-base',\n",
    "    tokenizer_name_or_path='t5-base',\n",
    "    max_seq_length=512,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.0,\n",
    "    adam_epsilon=1e-8,\n",
    "    warmup_steps=0,\n",
    "    train_batch_size=8,\n",
    "    eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    n_gpu=1,\n",
    "    early_stop_callback=False,\n",
    "    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
    "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72498dde-bbc6-4841-94af-604e77ade7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = EventDataset(tokenizer, '../data/AFG_data_quintuples/', 'val', 512)\n",
    "# len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93e12152-debd-4a95-a095-133e41aeaed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data = dataset[1550]\n",
    "# for i in range(20):\n",
    "#     data = dataset[i]\n",
    "#     # if tokenizer.decode(data['target_ids'])=='</s>':\n",
    "#     # print(tokenizer.decode(data['source_ids']))\n",
    "#     print(tokenizer.decode(data['target_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "689d5056-d91f-41f5-abd3-e7b63797db93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_dir': '../data/AFG_data_quintuples', 'output_dir': '../data/outputs', 'model_name_or_path': 't5-base', 'tokenizer_name_or_path': 't5-base', 'max_seq_length': 512, 'learning_rate': 0.0003, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'train_batch_size': 8, 'eval_batch_size': 8, 'num_train_epochs': 2, 'gradient_accumulation_steps': 16, 'n_gpu': 1, 'early_stop_callback': False, 'fp_16': False, 'opt_level': 'O1', 'max_grad_norm': 1.0, 'seed': 42}\n"
     ]
    }
   ],
   "source": [
    "args_dict.update({'data_dir': '../data/AFG_data_quintuples', 'output_dir': '../data/outputs', 'num_train_epochs':2})\n",
    "args = argparse.Namespace(**args_dict)\n",
    "print(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1a2ded7-150c-4221-891c-596e9f4b3b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=args.output_dir, monitor=\"val_loss\", mode=\"min\", save_top_k=5\n",
    ")\n",
    "early_stopping_callback = pl.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    devices=args.n_gpu,  # Use devices instead of gpus\n",
    "    accelerator='gpu' if args.n_gpu > 0 else 'cpu',  # Use accelerator\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    # early_stop_callback=False,\n",
    "    precision= 16 if args.fp_16 else 32,\n",
    "    # amp_level=args.opt_level,\n",
    "    gradient_clip_val=args.max_grad_norm,\n",
    "    # checkpoint_callback=checkpoint_callback,\n",
    "    callbacks=[LoggingCallback(),early_stopping_callback,checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba3d9aff-b99d-4b6f-9bb9-d979c10ea97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, type_path, args):\n",
    "    return EventDataset(tokenizer=tokenizer, data_dir=args.data_dir, type_path=type_path,  max_len=args.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2434ebdf-3b44-4f18-b2f2-8d9e7894b808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = T5FineTuner(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53e5ce91-3479-41f3-9d39-d80db15ebb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\desai\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74ca0e3-1c0b-4f4e-a00d-694b89309114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\desai\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                       | Params | Mode\n",
      "------------------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M  | eval\n",
      "------------------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1feb89f729a4beab75d50ea53fe6672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                               | 0/? [00:00<â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\desai\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
